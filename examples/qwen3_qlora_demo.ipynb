{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f789d7",
   "metadata": {},
   "source": [
    "# QLoRA Demo Notebook\n",
    "\n",
    "In this tutorial, we fine-tune the Qwen3 0.6B and 14B model using Low Rank Adaptation(LoRA), a parameter-efficient way of finetuning LLMs.\n",
    "\n",
    "LoRA works by freezing the original weights of the pre-trained model and\n",
    "injecting trainable low-rank matrices into each layer of the Transformer\n",
    "architecture. During fine-tuning, only these newly introduced low-rank matrices\n",
    "are updated, greatly decreasing the computational and memory resources required\n",
    "compared to traditional full fine-tuning. This approach is based on the\n",
    "observation that the changes in model weights needed for adaptation often have a\n",
    "low rank. The benefits of using LoRA include reduced GPU memory usage, faster\n",
    "training times, and the advantage that, after training, the LoRA adapters can be\n",
    "merged with the original model weights, resulting in no additional inference\n",
    "latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a40419",
   "metadata": {},
   "source": [
    "## Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kagglehub\n",
    "\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_datasets\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q grain\n",
    "!pip install -q git+https://github.com/google/tunix\n",
    "!pip install -q git+https://github.com/google/qwix\n",
    "\n",
    "!pip uninstall -q -y flax\n",
    "!pip install -q git+https://github.com/google/flax.git\n",
    "\n",
    "!pip install -q datasets\n",
    "!pip install safetensors\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to upload your metrics to Weights & Biases, please install the package and login. Make sure to install `wandb` before importing `tunix`.\n",
    "!pip install wandb\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498af0a7",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169bc305",
   "metadata": {},
   "source": [
    "### Deployment reference\n",
    "Below tables are references for the deployment. `min TPUs` is the TPU version and number of devices needed to run the corresponding model. `max_batch_size` is the maximum BATCH_SIZE to reach with `min TPUs`. `mesh` is how the model is placed and sharded on multiple TPUs if applicable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c6587",
   "metadata": {},
   "source": [
    "#### Training Full weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a174be",
   "metadata": {},
   "source": [
    "| model | Dtype | min TPUs | max_batch_size | mesh |\n",
    "|:----------|:----------|:----------|:----------|:----------|\n",
    "|qwen3_0.6b| bf16 | v2-1 |4|  N/A |\n",
    "|qwen3_0.6b| float32 | v2-2 |4| [(2, 1), (\"fsdp\", \"tp\")]  |\n",
    "|qwen3_0.6b | float32 |\tv2-2 |\t4 |\t[(1, 2), (\"fsdp\", \"tp\")] |\n",
    "|qwen3_14b |\tbf16 |\tv5e-8 |\t1 |\t[(8, 1), (\"fsdp\", \"tp\")]|\n",
    "|qwen3_14b | \tbf16 |\tv5e-8 | \t8 | \t[(1, 8), (\"fsdp\", \"tp\")]|\n",
    "|qwen3_14b|\tfloat32 |\tv5p-8 |\t32 |\t[(4, 1), (\"fsdp\", \"tp\")] |\n",
    "|qwen3_14b |\tfloat32 |\tv5p-8 |\t64 |\t[(1, 4), (\"fsdp\", \"tp\")]|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4db0e2",
   "metadata": {},
   "source": [
    "#### Training LoRA weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9878e3",
   "metadata": {},
   "source": [
    "| model | Dtype | min TPUs | max_batch_size | mesh |\n",
    "|:----------|:----------|:----------|:----------|:----------|\n",
    "|qwen3_0.6b |\tbf16 |\tv2-1 | \t8\t | N/A |\n",
    "|qwen3_0.6b | \tfloat32 |\tv2-1\t| 4\t | N/A |\n",
    "|qwen3_14b\t| bf16 | \tv5e-8 |\t1 |\t[(8, 1), (\"fsdp\", \"tp\")] |\n",
    "|qwen3_14b\t| bf16 |\tv5e-8 |\t32 |\t[(1, 8), (\"fsdp\", \"tp\")] |\n",
    "|qwen3_14b\t| float32 |\tv5e-8 |\t8 | \t[(1, 8), (\"fsdp\", \"tp\")] |\n",
    "|qwen3_14b\t| float32 |\tv5e-8 |\t1\t | [(8, 1), (\"fsdp\", \"tp\")] | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ff3c6",
   "metadata": {},
   "source": [
    "#### Traning QLoRA weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcbf905",
   "metadata": {},
   "source": [
    "| model | Dtype | min TPUs | max_batch_size | mesh |\n",
    "|:----------|:----------|:----------|:----------|:----------|\n",
    "|qwen3_0.6b |\tbf16 |\tv2-1 |\t8 | N/A|\t\n",
    "|qwen3_0.6b\t| float32 |\tv2-1 |\t4\t | N/A |\n",
    "|qwen3_14b\t| bf16\t| v5e-8 |\t16 | [(1, 8), (\"fsdp\", \"tp\")] |\t\n",
    "|qwen3_14b\t| bf16 | v5p-8 |\t128 | [(4, 1), (\"fsdp\", \"tp\")]\n",
    "|qwen3_14b\t| float32 |\tv5p-8 | \t64 | \t[(1, 4), (\"fsdp\", \"tp\")] |\n",
    "|qwen3_14b\t| float32 |\tv5p-8 |\t4\t | [(4, 1), (\"fsdp\", \"tp\")] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "BATCH_SIZE = 16 # Adjust based on your TPU memory and model size.\n",
    "\n",
    "# Model\n",
    "MESH = [(1, 1), (\"fsdp\", \"tp\")] # Adjust based on your TPU memory and model size.\n",
    "# LoRA\n",
    "RANK = 16\n",
    "ALPHA = 2.0\n",
    "\n",
    "# Train\n",
    "MAX_STEPS = 100\n",
    "EVAL_EVERY_N_STEPS = 20\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "\n",
    "# Checkpoint saving\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
    "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
    "PROFILING_DIR = \"/tmp/content/profiling/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a4ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "def create_dir(path):\n",
    "  try:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    logging.info(f\"Created dir: {path}\")\n",
    "  except OSError as e:\n",
    "    logging.error(f\"Error creating directory '{path}': {e}\")\n",
    "\n",
    "\n",
    "create_dir(INTERMEDIATE_CKPT_DIR)\n",
    "create_dir(CKPT_DIR)\n",
    "create_dir(PROFILING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82339b1a",
   "metadata": {},
   "source": [
    "# Download the weights from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d871ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Log in\n",
    "if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
    "  kagglehub.login()\n",
    "\n",
    "# alternatively place kaggle.json under ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facead42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "mesh = jax.make_mesh(*MESH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5dc7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flax import nnx\n",
    "import kagglehub\n",
    "from tunix.models.qwen3 import model\n",
    "from tunix.models.qwen3 import params\n",
    "import jax.numpy as jnp\n",
    "\n",
    "model_version = \"0.6b\"  # Change to \"14b\" for Qwen-3 14B\n",
    "MODEL_CONFIG = {\"0.6b\" : model.ModelConfig.qwen3_0_6b, \"14b\": model.ModelConfig.qwen3_14b}\n",
    "MODEL_CP_PATH = kagglehub.model_download(f\"qwen-lm/qwen-3/transformers/{model_version}\")\n",
    "\n",
    "config = MODEL_CONFIG[model_version]  # pick correponding config based on model version\n",
    "qwen3 = params.create_model_from_safe_tensors(MODEL_CP_PATH, config, mesh)\n",
    "nnx.display(qwen3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ad45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tunix.examples.data import translation_dataset as data_lib\n",
    "tokenizer = data_lib.HFTokenizer(\n",
    "    MODEL_CP_PATH,\n",
    "    add_bos=True,\n",
    "    add_eos=True,\n",
    "    hf_access_token=os.environ.get('T_HF_TOKEN'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b262af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import humanize\n",
    "def show_hbm_usage():\n",
    "  \"\"\"Displays memory usage per device.\"\"\"\n",
    "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
    "\n",
    "  for d in jax.local_devices():\n",
    "    stats = d.memory_stats()\n",
    "    used = stats[\"bytes_in_use\"]\n",
    "    limit = stats[\"bytes_limit\"]\n",
    "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939849bb",
   "metadata": {},
   "source": [
    "## Apply LoRA/QLoRA to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qwix\n",
    "def get_lora_model(base_model, mesh, quantize=False):\n",
    "  if quantize:\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "      module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj\",\n",
    "      rank=RANK,\n",
    "      alpha=ALPHA,\n",
    "      # comment the two args below for LoRA (w/o quantisation).\n",
    "      weight_qtype=\"nf4\",\n",
    "      tile_size=256,\n",
    "    )\n",
    "  else:\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "      module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj\",\n",
    "      rank=RANK,\n",
    "      alpha=ALPHA,\n",
    "    )\n",
    "\n",
    "  model_input = base_model.get_model_input()\n",
    "  lora_model = qwix.apply_lora_to_model(\n",
    "      base_model, lora_provider, **model_input\n",
    "  )\n",
    "\n",
    "  with mesh:\n",
    "    state = nnx.state(lora_model)\n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "    nnx.update(lora_model, sharded_state)\n",
    "\n",
    "  return lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae90bc3",
   "metadata": {},
   "source": [
    "## Load Datasets for SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the training and validation datasets\n",
    "\n",
    "from tunix.examples.data import translation_dataset as data_lib\n",
    "from tunix.rl import common\n",
    "from tunix.sft import peft_trainer\n",
    "\n",
    "train_ds, validation_ds = data_lib.create_datasets(\n",
    "    dataset_name='mtnt/en-fr',\n",
    "    # Uncomment the line below to use a Hugging Face dataset.\n",
    "    # Note that this requires upgrading the 'datasets' package and restarting\n",
    "    # the Colab runtime.\n",
    "    # dataset_name='Helsinki-NLP/opus-100',\n",
    "    global_batch_size=BATCH_SIZE,\n",
    "    max_target_length=256,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "def gen_model_input_fn(x: peft_trainer.TrainingInput):\n",
    "  pad_mask = x.input_tokens != tokenizer.pad_id()\n",
    "  positions = common.build_positions_from_mask(pad_mask)\n",
    "  attention_mask = common.make_causal_attn_mask(pad_mask)\n",
    "  return {\n",
    "      'input_tokens': x.input_tokens,\n",
    "      'input_mask': x.input_mask,\n",
    "      'positions': positions,\n",
    "      'attention_mask': attention_mask,\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df7f39",
   "metadata": {},
   "source": [
    "## SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a45d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tunix.sft import metrics_logger\n",
    "\n",
    "import optax\n",
    "\n",
    "logging_option = metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=\"/tmp/tensorboard/full\", flush_every_n_steps=20\n",
    ")\n",
    "training_config = peft_trainer.TrainingConfig(\n",
    "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    metrics_logging_options=logging_option,\n",
    ")\n",
    "trainer = peft_trainer.PeftTrainer(qwen3, optax.adamw(1e-5), training_config)\n",
    "trainer = trainer.with_gen_model_input_fn(gen_model_input_fn)\n",
    "\n",
    "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"full_training\")):\n",
    "  with mesh:\n",
    "    trainer.train(train_ds, validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36401482",
   "metadata": {},
   "source": [
    "### Training with LoRA/QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA model\n",
    "lora_qwen3 = get_lora_model(qwen3, mesh=mesh)\n",
    "nnx.display(lora_qwen3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b88f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since LoRA model is sharing backbone with base model,\n",
    "# restart Colab runtime so base model is loaded as pre-trained.\n",
    "\n",
    "training_config = peft_trainer.TrainingConfig(\n",
    "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    checkpoint_root_directory=CKPT_DIR,\n",
    ")\n",
    "lora_trainer = peft_trainer.PeftTrainer(\n",
    "    lora_qwen3, optax.adamw(1e-3), training_config\n",
    ").with_gen_model_input_fn(gen_model_input_fn)\n",
    "\n",
    "print(\"Start to train lora.\")\n",
    "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"peft\")):\n",
    "  with mesh:\n",
    "    lora_trainer.train(train_ds, validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qlora model\n",
    "lora_qwen3_quant = get_lora_model(qwen3, mesh=mesh, quantize=True)\n",
    "nnx.display(lora_qwen3_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = peft_trainer.TrainingConfig(\n",
    "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    checkpoint_root_directory=CKPT_DIR,\n",
    ")\n",
    "qlora_trainer = peft_trainer.PeftTrainer(\n",
    "    lora_qwen3_quant, optax.adamw(1e-3), training_config\n",
    ").with_gen_model_input_fn(gen_model_input_fn)\n",
    "\n",
    "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"peft\")):\n",
    "  with mesh:\n",
    "    qlora_trainer.train(train_ds, validation_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
