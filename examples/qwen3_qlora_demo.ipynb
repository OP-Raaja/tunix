{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f789d7",
   "metadata": {},
   "source": [
    "# QLoRA Demo Notebook\n",
    "\n",
    "In this tutorial, we fine-tune the Qwen3 2B and 14B model using Low Rank Adaptation(LoRA), a parameter-efficient way of finetuning LLMs.\n",
    "\n",
    "LoRA works by freezing the original weights of the pre-trained model and\n",
    "injecting trainable low-rank matrices into each layer of the Transformer\n",
    "architecture. During fine-tuning, only these newly introduced low-rank matrices\n",
    "are updated, greatly decreasing the computational and memory resources required\n",
    "compared to traditional full fine-tuning. This approach is based on the\n",
    "observation that the changes in model weights needed for adaptation often have a\n",
    "low rank. The benefits of using LoRA include reduced GPU memory usage, faster\n",
    "training times, and the advantage that, after training, the LoRA adapters can be\n",
    "merged with the original model weights, resulting in no additional inference\n",
    "latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a40419",
   "metadata": {},
   "source": [
    "## Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168ca4e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q kagglehub\n",
    "\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q grain\n",
    "!pip install -q git+https://github.com/google/tunix\n",
    "!pip install -q git+https://github.com/google/qwix\n",
    "\n",
    "!pip uninstall -q -y flax\n",
    "!pip install -q git+https://github.com/google/flax.git\n",
    "\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86806e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# If you want to upload your metrics to Weights & Biases, please install the package and login. Make sure to install `wandb` before importing `tunix`.\n",
    "!pip install wandb\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498af0a7",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0e9f4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Model\n",
    "MESH = [(1, 1), (\"fsdp\", \"tp\")]\n",
    "# LoRA\n",
    "RANK = 16\n",
    "ALPHA = 2.0\n",
    "\n",
    "# Train\n",
    "MAX_STEPS = 100\n",
    "EVAL_EVERY_N_STEPS = 20\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "\n",
    "# Checkpoint saving\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
    "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
    "PROFILING_DIR = \"/tmp/content/profiling/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a4ed5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "def create_dir(path):\n",
    "  try:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    logging.info(f\"Created dir: {path}\")\n",
    "  except OSError as e:\n",
    "    logging.error(f\"Error creating directory '{path}': {e}\")\n",
    "\n",
    "\n",
    "create_dir(INTERMEDIATE_CKPT_DIR)\n",
    "create_dir(CKPT_DIR)\n",
    "create_dir(PROFILING_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82339b1a",
   "metadata": {},
   "source": [
    "# Download the weights from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d871ed0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Log in\n",
    "if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
    "  kagglehub.login()\n",
    "\n",
    "# alternatively place kaggle.json under ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facead42",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "mesh = jax.make_mesh(*MESH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5dc7ab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import kagglehub\n",
    "from tunix.models.qwen3 import model\n",
    "from tunix.models.qwen3 import params\n",
    "\n",
    "MODEL_CP_PATH = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "\n",
    "config = (\n",
    "    model.ModelConfig.qwen3_0_6b()\n",
    ")  # pick correponding config based on model version\n",
    "qwen3 = params.create_model_from_safe_tensors(MODEL_CP_PATH, config, mesh)\n",
    "nnx.display(qwen3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ad45f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662dc714",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def templatize(prompts):\n",
    "  out = []\n",
    "  for p in prompts:\n",
    "    out.append(\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": p},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=True,\n",
    "        )\n",
    "    )\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2baaa74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tunix.generate import sampler\n",
    "\n",
    "inputs = templatize([\n",
    "    \"which is larger 9.9 or 9.11?\",\n",
    "    \"如何制作月饼?\",\n",
    "    \"tell me your name, respond in Chinese\",\n",
    "])\n",
    "\n",
    "sampler = sampler.Sampler(\n",
    "    qwen3,\n",
    "    tokenizer,\n",
    "    sampler.CacheConfig(\n",
    "        cache_size=256, num_layers=28, num_kv_heads=8, head_dim=128\n",
    "    ),\n",
    ")\n",
    "out = sampler(inputs, max_generation_steps=128, echo=True)\n",
    "\n",
    "for t in out.text:\n",
    "  print(t)\n",
    "  print(\"*\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939849bb",
   "metadata": {},
   "source": [
    "## Apply LoRA/QLoRA to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0f42a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import qwix\n",
    "def get_lora_model(base_model, mesh):\n",
    "  lora_provider = qwix.LoraProvider(\n",
    "      module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj\",\n",
    "      rank=RANK,\n",
    "      alpha=ALPHA,\n",
    "      # comment the two args below for LoRA (w/o quantisation).\n",
    "      weight_qtype=\"nf4\",\n",
    "      tile_size=256,\n",
    "  )\n",
    "\n",
    "  model_input = base_model.get_model_input()\n",
    "  lora_model = qwix.apply_lora_to_model(\n",
    "      base_model, lora_provider, **model_input\n",
    "  )\n",
    "\n",
    "  with mesh:\n",
    "    state = nnx.state(lora_model)\n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "    nnx.update(lora_model, sharded_state)\n",
    "\n",
    "  return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f2ae5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# LoRA model\n",
    "lora_qwen3 = get_lora_model(qwen3, mesh=mesh)\n",
    "nnx.display(lora_qwen3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae90bc3",
   "metadata": {},
   "source": [
    "## Load Datasets for SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a1361",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Loads the training and validation datasets\n",
    "\n",
    "from tunix.examples.data import translation_dataset as data_lib\n",
    "from tunix.rl import common\n",
    "from tunix.sft import peft_trainer\n",
    "\n",
    "train_ds, validation_ds = data_lib.create_datasets(\n",
    "    dataset_name='mtnt/en-fr',\n",
    "    # Uncomment the line below to use a Hugging Face dataset.\n",
    "    # Note that this requires upgrading the 'datasets' package and restarting\n",
    "    # the Colab runtime.\n",
    "    # dataset_name='Helsinki-NLP/opus-100',\n",
    "    global_batch_size=BATCH_SIZE,\n",
    "    max_target_length=256,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "def gen_model_input_fn(x: peft_trainer.TrainingInput):\n",
    "  pad_mask = x.input_tokens != tokenizer.pad_id()\n",
    "  positions = common.build_positions_from_mask(pad_mask)\n",
    "  attention_mask = common.make_causal_attn_mask(pad_mask)\n",
    "  return {\n",
    "      'input_tokens': x.input_tokens,\n",
    "      'input_mask': x.input_mask,\n",
    "      'positions': positions,\n",
    "      'attention_mask': attention_mask,\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df7f39",
   "metadata": {},
   "source": [
    "## SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a45d5f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tunix.sft import metrics_logger\n",
    "\n",
    "import optax\n",
    "\n",
    "logging_option = metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=\"/tmp/tensorboard/full\", flush_every_n_steps=20\n",
    ")\n",
    "training_config = peft_trainer.TrainingConfig(\n",
    "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    metrics_logging_options=logging_option,\n",
    ")\n",
    "trainer = peft_trainer.PeftTrainer(qwen3, optax.adamw(1e-5), training_config)\n",
    "trainer = trainer.with_gen_model_input_fn(gen_model_input_fn)\n",
    "\n",
    "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"full_training\")):\n",
    "  with mesh:\n",
    "    trainer.train(train_ds, validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36401482",
   "metadata": {},
   "source": [
    "### Training with LoRA/QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b88f15",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Since LoRA model is sharing backbone with base model,\n",
    "# restart Colab runtime so base model is loaded as pre-trained.\n",
    "\n",
    "training_config = peft_trainer.TrainingConfig(\n",
    "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    checkpoint_root_directory=CKPT_DIR,\n",
    ")\n",
    "lora_trainer = peft_trainer.PeftTrainer(\n",
    "    lora_gemma, optax.adamw(1e-3), training_config\n",
    ").with_gen_model_input_fn(gen_model_input_fn)\n",
    "\n",
    "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"peft\")):\n",
    "  with mesh:\n",
    "    lora_trainer.train(train_ds, validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfef94e",
   "metadata": {},
   "source": [
    "## Generate with the LoRA/QLoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4a2da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tunix.generate import sampler\n",
    "\n",
    "inputs = templatize([\n",
    "    \"which is larger 9.9 or 9.11?\",\n",
    "    \"如何制作月饼?\",\n",
    "    \"tell me your name, respond in Chinese\",\n",
    "])\n",
    "\n",
    "sampler = sampler.Sampler(\n",
    "    lora_qwen3,\n",
    "    tokenizer,\n",
    "    sampler.CacheConfig(\n",
    "        cache_size=256, num_layers=28, num_kv_heads=8, head_dim=128\n",
    "    ),\n",
    ")\n",
    "out = sampler(inputs, max_generation_steps=128, echo=True)\n",
    "\n",
    "for t in out.text:\n",
    "  print(t)\n",
    "  print(\"*\" * 30)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
