{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-gq1gcpyCpT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/tunix/blob/main/examples/qlora_demo.ipynb\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "## Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RTlz7JP7yCpT"
      },
      "outputs": [],
      "source": [
        "# !pip install -q kagglehub\n",
        "\n",
        "# !pip install -q tensorflow\n",
        "# !pip install -q tensorboardX\n",
        "# !pip install -q grain\n",
        "# !pip install -q datasets\n",
        "# !pip install -q git+https://github.com/google/tunix\n",
        "# !pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "# !pip uninstall -q -y flax\n",
        "# !pip install -q git+https://github.com/google/flax.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEjoS0-JyCpT"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LshEMmSzx6W6"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import time\n",
        "\n",
        "from flax import nnx\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "from qwix import lora\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma import data as data_lib\n",
        "from tunix.models.gemma import gemma as gemma_lib\n",
        "from tunix.models.gemma import params as params_lib\n",
        "from tunix.sft import metrics_logger\n",
        "from tunix.sft import peft_trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnEZ_jXwypn-"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QBcMaL22T3Uu"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Model\n",
        "MESH = [(1, 8), (\"fsdp\", \"tp\")]\n",
        "# LoRA\n",
        "RANK = 16\n",
        "ALPHA = 2.0\n",
        "\n",
        "# Train\n",
        "MAX_STEPS = 100\n",
        "EVAL_EVERY_N_STEPS = 20\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"/content/ckpts/\"\n",
        "PROFILING_DIR = \"/content/profiling/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3s5Qg6xT3Uu"
      },
      "source": [
        "## Load Gemma 2B\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
        "to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.com/models/google/gemma/flax/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o_7Sk8d7T3Uu"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c80f8a5ad6c42f0b1416df2a4347c16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggleâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Log in\n",
        "if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "  kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oJC3Hfh9T3Uv"
      },
      "outputs": [
        {
          "ename": "KaggleApiHTTPError",
          "evalue": "403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/google/gemma/flax/2b/2. The server reported the following issues: Permission denied on resource (or it may not exists).\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/exceptions.py:66\u001b[39m, in \u001b[36mkaggle_api_raise_for_status\u001b[39m\u001b[34m(response, resource_handle)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/models/google/gemma/flax/2b/2/files?page_size=25",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mKaggleApiHTTPError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m kaggle_ckpt_path = \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_download\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/gemma/flax/2b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/models.py:35\u001b[39m, in \u001b[36mmodel_download\u001b[39m\u001b[34m(handle, path, force_download)\u001b[39m\n\u001b[32m     33\u001b[39m h = parse_model_handle(handle)\n\u001b[32m     34\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh.to_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m, extra={**EXTRA_CONSOLE_BLOCK})\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m path, _ = \u001b[43mregistry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/registry.py:28\u001b[39m, in \u001b[36mMultiImplRegistry.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m._impls):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m impl.is_supported(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     30\u001b[39m         fails.append(\u001b[38;5;28mtype\u001b[39m(impl).\u001b[34m__name__\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/resolver.py:29\u001b[39m, in \u001b[36mResolver.__call__\u001b[39m\u001b[34m(self, handle, path, force_download)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mself\u001b[39m, handle: T, path: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, *, force_download: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m     18\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resolves a handle into a path with the requested file(s) and the resource's version number.\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m \u001b[33;03m        Some cases where version number might be missing: Competition datasource, API-based models.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     path, version = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[39;00m\n\u001b[32m     32\u001b[39m     register_datasource_access(handle, version)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/http_resolver.py:172\u001b[39m, in \u001b[36mModelHttpResolver._resolve\u001b[39m\u001b[34m(self, h, path, force_download)\u001b[39m\n\u001b[32m    167\u001b[39m     api_client.download_file(url_path, out_path, h, extract_auto_compressed_file=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# List the files and decide how to download them:\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# - <= 25 files: Download files in parallel\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# > 25 files: Download the archive and uncompress\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     (files, has_more) = \u001b[43m_list_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_more:\n\u001b[32m    174\u001b[39m         \u001b[38;5;66;03m# Downloading the full archived bundle.\u001b[39;00m\n\u001b[32m    175\u001b[39m         archive_path = get_cached_archive_path(h)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/http_resolver.py:311\u001b[39m, in \u001b[36m_list_files\u001b[39m\u001b[34m(api_client, h)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_list_files\u001b[39m(api_client: KaggleApiV1Client, h: ModelHandle) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     json_response = \u001b[43mapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_build_list_model_instance_version_files_url_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mfiles\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_response:\n\u001b[32m    313\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mInvalid ListModelInstanceVersionFiles API response. Expected to include a \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfiles\u001b[39m\u001b[33m'\u001b[39m\u001b[33m field\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/clients.py:139\u001b[39m, in \u001b[36mKaggleApiV1Client.get\u001b[39m\u001b[34m(self, path, resource_handle)\u001b[39m\n\u001b[32m    132\u001b[39m url = \u001b[38;5;28mself\u001b[39m._build_url(path)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m requests.get(\n\u001b[32m    134\u001b[39m     url,\n\u001b[32m    135\u001b[39m     headers={\u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: get_user_agent()},\n\u001b[32m    136\u001b[39m     auth=\u001b[38;5;28mself\u001b[39m._get_auth(),\n\u001b[32m    137\u001b[39m     timeout=(DEFAULT_CONNECT_TIMEOUT, DEFAULT_READ_TIMEOUT),\n\u001b[32m    138\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[43mkaggle_api_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_for_version_update(response)\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/kagglehub/exceptions.py:106\u001b[39m, in \u001b[36mkaggle_api_raise_for_status\u001b[39m\u001b[34m(response, resource_handle)\u001b[39m\n\u001b[32m     97\u001b[39m     message = (\n\u001b[32m     98\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease make sure you specified the correct resource identifiers.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    103\u001b[39m     )\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Default handling\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m KaggleApiHTTPError(message, response=response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[31mKaggleApiHTTPError\u001b[39m: 403 Client Error.\n\nYou don't have permission to access resource at URL: https://www.kaggle.com/models/google/gemma/flax/2b/2. The server reported the following issues: Permission denied on resource (or it may not exists).\nPlease make sure you are authenticated if you are trying to access a private resource or a resource requiring consent."
          ]
        }
      ],
      "source": [
        "kaggle_ckpt_path = kagglehub.model_download(\"google/gemma/flax/2b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_5FWrq-T3Uv"
      },
      "outputs": [],
      "source": [
        "# This is a workaround. The checkpoints on Kaggle don't work with NNX. So, we\n",
        "# load the model, save the checkpoint locally, and then reload the model\n",
        "# (sharded).\n",
        "params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"2b\"))\n",
        "gemma = gemma_lib.Transformer.from_params(params, version=\"2b\")\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "_, state = nnx.split(gemma)\n",
        "checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ertAr6FT3Uv"
      },
      "outputs": [],
      "source": [
        "# Wait for the ckpt to save successfully.\n",
        "time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGYKuLyFT3Uv"
      },
      "outputs": [],
      "source": [
        "# Delete the intermediate model to save memory.\n",
        "del params\n",
        "del gemma\n",
        "del state\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# add the parent directory (one level up) to sys.path\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../../maxtext')))\n",
        "\n",
        "# ! pip install -r ../../maxtext/requirements.txt\n",
        "\n",
        "import MaxText as mt\n",
        "from MaxText import pyconfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_ref_maxtext_model():\n",
        "\n",
        "  #python3 -m MaxText.train MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIRECTORY} dataset_path=${DATASET_PATH} tokenizer_path=assets/tokenizer.gemma load_parameters_path=${CONVERTED_CHECKPOINT} per_device_batch_size=1 run_name=${FINETUNE_RUN_NAME} max_target_length=8192 steps=10 async_checkpointing=false model_name=gemma-2b checkpoint_period=5\n",
        "\n",
        "  #TODO: @mazumdera: change this to use Gemma2-2b-it\n",
        "  config = pyconfig.initialize(\n",
        "      [\"\", \"../../maxtext/MaxText/configs/base.yml\"], #TODO: @mazumdera: why decode.py?\n",
        "      base_output_directory=\"gs://dummy_output_dir\",  # This is not used in Tunix.\n",
        "      run_name=\"test-tunix-maxtext-gemma-2b\",\n",
        "      # dataset_path=we use Tunix's dataset\n",
        "      load_parameters_path=\"gs://maxtext-gemma/2b/\",\n",
        "      tokenizer_path=\"../../maxtext/assets/tokenizer.gemma\",\n",
        "      per_device_batch_size=1,\n",
        "      max_target_length=8192,\n",
        "      steps=10,\n",
        "      async_checkpointing=\"false\",\n",
        "      model_name=\"gemma-2b\",\n",
        "      checkpoint_period=5,\n",
        "      skip_jax_distributed_system=\"true\"\n",
        "\n",
        "  )\n",
        "  model = mt.from_pretrained(config)\n",
        "  mesh  = model.mesh\n",
        "  \n",
        "  # We can continue to use Tunix's model_config\n",
        "  model_config = gemma_lib.TransformerConfig.gemma2_2b()\n",
        "  \n",
        "  return model, mesh, model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFKUZT10T3Uv"
      },
      "outputs": [],
      "source": [
        "def get_base_model(ckpt_path):\n",
        "\n",
        "  model_config = gemma_lib.TransformerConfig.gemma_2b()\n",
        "  mesh = jax.make_mesh(*MESH)\n",
        "  abs_gemma: nnx.Module = nnx.eval_shape(\n",
        "      lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
        "  )\n",
        "  abs_state = nnx.state(abs_gemma)\n",
        "  abs_state = jax.tree.map(\n",
        "      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
        "      abs_state,\n",
        "      nnx.get_named_sharding(abs_state, mesh),\n",
        "  )\n",
        "  checkpointer = ocp.StandardCheckpointer()\n",
        "  restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
        "\n",
        "  graph_def, _ = nnx.split(abs_gemma)\n",
        "  gemma = nnx.merge(graph_def, restored_params)\n",
        "  return gemma, mesh, model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCPPEEi3T3Uv"
      },
      "outputs": [],
      "source": [
        "# Base model\n",
        "# gemma, mesh, model_config = get_base_model(\n",
        "#     ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
        "# )\n",
        "\n",
        "gemma, mesh, model_config = get_ref_maxtext_model()\n",
        "\n",
        "nnx.display(gemma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2iDYbnsT3Uv"
      },
      "source": [
        "## Prompt the model\n",
        "\n",
        "Let's see how the model performs on the English-French translation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH82cHpAT3Uv"
      },
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "    os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
        ")\n",
        "\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "input_batch = [\n",
        "    \"Translate this into French:\\nHello, my name is Morgane.\\n\",\n",
        "    \"Translate this into French:\\nThis dish is delicious!\\n\",\n",
        "    \"Translate this into French:\\nI am a student.\\n\",\n",
        "    \"Translate this into French:\\nHow's the weather today?\\n\",\n",
        "]\n",
        "\n",
        "out_data = sampler(\n",
        "    input_strings=input_batch,\n",
        "    total_generation_steps=10,  # The number of steps performed when generating a response.\n",
        ")\n",
        "\n",
        "for input_string, out_string in zip(input_batch, out_data.text):\n",
        "  print(f\"----------------------\")\n",
        "  print(f\"Prompt:\\n{input_string}\")\n",
        "  print(f\"Output:\\n{out_string}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdxn7DQYT3Uv"
      },
      "source": [
        "## Apply LoRA/QLoRA to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3t-7leAT3Uv"
      },
      "outputs": [],
      "source": [
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = lora.LoraProvider(\n",
        "      module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj\",\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "      # comment the two args below for LoRA (w/o quantisation).\n",
        "      weight_qtype=\"nf4\",\n",
        "      tile_size=256,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = lora.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  with mesh:\n",
        "    state = nnx.state(lora_model)\n",
        "    pspecs = nnx.get_partition_spec(state)\n",
        "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "    nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfmodoqrT3Uv"
      },
      "outputs": [],
      "source": [
        "# LoRA model\n",
        "lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "nnx.display(lora_gemma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD6-iU0PT3Uv"
      },
      "source": [
        "## Load Datasets for SFT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T_7Cik8T3Uv"
      },
      "outputs": [],
      "source": [
        "# Loads the training and validation datasets\n",
        "train_ds, validation_ds = data_lib.create_datasets(\n",
        "    dataset_name='mtnt/en-fr',\n",
        "    # Uncomment the line below to use a Hugging Face dataset.\n",
        "    # Note that this requires upgrading the 'datasets' package and restarting\n",
        "    # the Colab runtime.\n",
        "    # dataset_name='Helsinki-NLP/opus-100',\n",
        "    global_batch_size=BATCH_SIZE,\n",
        "    max_target_length=256,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    tokenizer=gemma_tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "def gen_model_input_fn(x: peft_trainer.TrainingInput):\n",
        "  pad_mask = x.input_tokens != gemma_tokenizer.pad_id()\n",
        "  positions = gemma_lib.build_positions_from_mask(pad_mask)\n",
        "  attention_mask = gemma_lib.make_causal_attn_mask(pad_mask)\n",
        "  return {\n",
        "      'input_tokens': x.input_tokens,\n",
        "      'input_mask': x.input_mask,\n",
        "      'positions': positions,\n",
        "      'attention_mask': attention_mask,\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w0PHlVBT3Uv"
      },
      "source": [
        "## SFT Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh1xPieRT3Uv"
      },
      "source": [
        "### Training with full weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oYR9JKNT3Uv"
      },
      "outputs": [],
      "source": [
        "logging_option = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/tmp/tensorboard/full\", flush_every_n_steps=20\n",
        ")\n",
        "training_config = peft_trainer.TrainingConfig(\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    metrics_logging_options=logging_option,\n",
        ")\n",
        "trainer = peft_trainer.PeftTrainer(gemma, optax.adamw(1e-5), training_config)\n",
        "trainer = trainer.with_gen_model_input_fn(gen_model_input_fn)\n",
        "\n",
        "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"full_training\")):\n",
        "  with mesh:\n",
        "    trainer.train(train_ds, validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gd-66SRT3Uv"
      },
      "source": [
        "### Training with LoRA/QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZVp0SYMT3Uv"
      },
      "outputs": [],
      "source": [
        "# Restart Colab runtime.\n",
        "\n",
        "training_config = peft_trainer.TrainingConfig(\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    checkpoint_root_directory=CKPT_DIR,\n",
        ")\n",
        "lora_trainer = peft_trainer.PeftTrainer(\n",
        "    lora_gemma, optax.adamw(1e-3), training_config\n",
        ").with_gen_model_input_fn(gen_model_input_fn)\n",
        "\n",
        "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"peft\")):\n",
        "  with mesh:\n",
        "    lora_trainer.train(train_ds, validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXUzmyVIT3Uv"
      },
      "source": [
        "### Compare profile results of different training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIw2rIKmT3Uv"
      },
      "source": [
        "<font size=3>Setup<font>           | <font size=3>Train Step Time<font> | <font size=3>Peak Memory Usage<font>\n",
        "---------------------------------- | ---------------------------------- | ------------------------------\n",
        "<font size=3>Full weights<font>        |   <font size=3>~1.22 s<font>     |   <font size=3>43.26 GiB<font>\n",
        "<font size=3>QLoRA<font>        |   <font size=3>~1.19 s<font>     |   <font size=3>28.14 GiB<font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QICLDHTkT3Uv"
      },
      "source": [
        "## Generate with the LoRA/QLoRA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTsmUCK6T3Uv"
      },
      "source": [
        "The QLoRA model still cannot do English-to-French translation properly since we\n",
        "only trained for 100 steps. If you train it for longer, you will see better\n",
        "results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WohZCuY0T3Uw"
      },
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "    os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
        ")\n",
        "\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "input_batch = [\n",
        "    \"Translate this into French:\\nHello, my name is Morgane.\\n\",\n",
        "    \"Translate this into French:\\nThis dish is delicious!\\n\",\n",
        "    \"Translate this into French:\\nI am a student.\\n\",\n",
        "    \"Translate this into French:\\nHow's the weather today?\\n\",\n",
        "]\n",
        "\n",
        "out_data = sampler(\n",
        "    input_strings=input_batch,\n",
        "    total_generation_steps=10,  # The number of steps performed when generating a response.\n",
        ")\n",
        "\n",
        "for input_string, out_string in zip(input_batch, out_data.text):\n",
        "  print(f\"----------------------\")\n",
        "  print(f\"Prompt:\\n{input_string}\")\n",
        "  print(f\"Output:\\n{out_string}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//third_party/py/tunix/google/examples/qlora_gemma:qlora_demo_colab",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/py/tunix/google/examples/qlora_gemma/qlora_demo.ipynb",
          "timestamp": 1745566103252
        }
      ]
    },
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
