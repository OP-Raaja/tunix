{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "abdhOBYHqYz6",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/tunix/blob/main/examples/grpo_demo.ipynb\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This tutorial demonstrates training the Gemma 2 2B-IT model on the GSM8K math\n",
        "reasoning benchmark using Group Relative Policy Optimization (GRPO). GRPO can\n",
        "enhance your model's problem-solving skills on mathematical word problems,\n",
        "coding problems, etc.\n",
        "\n",
        "GRPO is an RL algorithm designed to enhance the reasoning abilities of LLMs. It\n",
        "is a variant of Proximal Policy Optimization (PPO) that reduces memory usage by\n",
        "eliminating the need for a separate value function model. GRPO works by\n",
        "generating multiple responses for a given prompt, evaluating these responses\n",
        "using a reward model, and then calculating a relative advantage based on the\n",
        "group's performance to update the policy.\n",
        "\n",
        "In this tutorial we use Colab's `v2-8` TPU. Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afofSj37qYz6",
      "metadata": {},
      "source": [
        "## Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z03GnyApTn1j",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "# !pip install -q tensorflow\n",
        "# !pip install -q tensorboardX\n",
        "# !pip install -q grain\n",
        "# !pip install --force-reinstall \"jax==0.6.2\" \"jaxlib==0.6.2\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "# !pip install \"jax[tpu]==0.7.1.dev20250813\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "# !pip install -q git+https://github.com/google/tunix\n",
        "!pip install -e ~/tunix/\n",
        "# !pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "# !pip install -q git+https://github.com/google/flax.git\n",
        "!pip install -q git+https://github.com/google/flax.git@7a429f33fca2179079f163934a11658f6ddcb039\n",
        "# !pip install -q tensorflow-datasets\n",
        "\n",
        "# !pip install -q git+https://github.com/AI-Hypercomputer/pathways-utils.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f95eb96c",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LnF9ZACiTn1k",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "McTNo_r8Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "import functools\n",
        "import gc\n",
        "import os\n",
        "from pprint import pprint\n",
        "import re\n",
        "import time\n",
        "\n",
        "from flax import nnx\n",
        "import grain\n",
        "import humanize\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "import qwix\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm.auto import tqdm\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma import data as data_lib\n",
        "from tunix.models.gemma import gemma as gemma_lib\n",
        "from tunix.models.gemma import params as params_lib\n",
        "from tunix.rl import rl_cluster as rl_cluster_lib\n",
        "from tunix.rl.rollout import base_rollout\n",
        "from tunix.rl.grpo.grpo_learner import GrpoConfig, GrpoLearner\n",
        "from tunix.sft import metrics_logger\n",
        "\n",
        "os.environ['TPU_LIBRARY_PATH'] = '/home/linchai_google_com/miniconda3/envs/vllm/lib/python3.12/site-packages/libtpu/libtpu.so'\n",
        "os.environ['SKIP_JAX_PRECOMPILE'] = '1'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0daa3fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Eu_NI9nHTn1k",
      "metadata": {},
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Let's define the configuration we are going to use. Note that this is by no\n",
        "means a \"perfect\" set of hyperparameters. To get good results, you might have\n",
        "to train the model for longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZPPKme47Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ====== Data ======\n",
        "TRAIN_DATA_DIR = \"./data/train\"\n",
        "TEST_DATA_DIR = \"./data/test\"\n",
        "TRAIN_FRACTION = 1.0\n",
        "\n",
        "# ====== Reproducibility ======\n",
        "SEED = 42\n",
        "\n",
        "# ====== LoRA ======\n",
        "RANK = 64\n",
        "ALPHA = 64.0\n",
        "\n",
        "# ====== Sharding ======\n",
        "MESH = [(1, 4), (\"fsdp\", \"tp\")]\n",
        "\n",
        "# ====== GRPO ======\n",
        "# === Generation during GRPO training ===\n",
        "MAX_PROMPT_LENGTH = 256\n",
        "TOTAL_GENERATION_STEPS = 768\n",
        "# Important to keep a high-ish temperature for varied, diverse responses during\n",
        "# training.\n",
        "TEMPERATURE = 0.9\n",
        "TOP_P = 1.0\n",
        "TOP_K = 50\n",
        "# The number of times the policy generates multiple responses for a given prompt\n",
        "# within a single training step. This corresponds to `G` in Algorithm 1 in the\n",
        "# paper. The \"group\" in GRPO comes from here.\n",
        "NUM_GENERATIONS = 2\n",
        "\n",
        "# === other GRPO configs ===\n",
        "# The number of iterations per batch (ùúá in GRPO algo 1).\n",
        "NUM_ITERATIONS = 1\n",
        "# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.\n",
        "# Important to keep a high enough value for this, otherwise, the KL divergence\n",
        "# can increase unchecked.\n",
        "BETA = 0.08\n",
        "# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for\n",
        "# stable updates.\n",
        "EPSILON = 0.2\n",
        "\n",
        "# ====== Training ======\n",
        "BATCH_SIZE = 1\n",
        "# Increase `NUM_BATCHES` and `MAX_STEPS` for better results.\n",
        "# NUM_BATCHES = 3738\n",
        "NUM_BATCHES = 4\n",
        "# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n",
        "# increased to a max. of 330 (if batch size is 4).\n",
        "NUM_TEST_BATCHES = 5 #100 #Anisha: making it small for quick eval\n",
        "\n",
        "EVAL_EVERY_N_STEPS = 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\n",
        "NUM_EPOCHS = 1  # can potentially train for more epochs\n",
        "\n",
        "# Number of training steps.\n",
        "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
        "\n",
        "# === AdamW, warmup, cosine scheduler ===\n",
        "LEARNING_RATE = 3e-6\n",
        "B1 = 0.9\n",
        "B2 = 0.99\n",
        "WEIGHT_DECAY = 0.1\n",
        "# == Cosine decay with warmup scheduler ==\n",
        "# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n",
        "# steps, and then gradually decrease the learning rate to 0 using cosine\n",
        "# scheduler.\n",
        "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
        "# == Grad clipping ==\n",
        "# Grad clipping to prevent large gradients. Found this\n",
        "# important to keep KL divergence in check.\n",
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"/home/linchai_google_com/content/intermediate_ckpt_llama3/\"\n",
        "CKPT_DIR = \"/home/linchai_google_com/content/ckpts_llama3/\"\n",
        "SAVE_INTERVAL_STEPS = 500\n",
        "MAX_TO_KEEP = 4\n",
        "\n",
        "# ====== Inference ======\n",
        "GENERATION_CONFIGS = {\n",
        "    # greedy search\n",
        "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
        "    # some randomness\n",
        "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "    # liberal\n",
        "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ngjtE-63Tn1k",
      "metadata": {},
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wjMFOr7aTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_hbm_usage():\n",
        "  \"\"\"Displays memory usage per device.\"\"\"\n",
        "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
        "\n",
        "  for d in jax.local_devices():\n",
        "    stats = d.memory_stats()\n",
        "    used = stats[\"bytes_in_use\"]\n",
        "    limit = stats[\"bytes_limit\"]\n",
        "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6BtpYMlaTn1k",
      "metadata": {},
      "source": [
        "## Data preprocessing\n",
        "\n",
        "First, let's define some special tokens. We instruct the model to first reason\n",
        "between the `<reasoning>` and `</reasoning>` tokens. After\n",
        "reasoning, we expect it to provide the answer between the `<answer>` and\n",
        "`</answer>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h6RGv1kSTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "model_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
        "\n",
        "reasoning_start = \"<reasoning>\"\n",
        "reasoning_end = \"</reasoning>\"\n",
        "solution_start = \"<answer>\"\n",
        "solution_end = \"</answer>\"\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\n",
        "provide your reasoning. Place it between {reasoning_start} and \\\n",
        "{reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\n",
        "value) between {solution_start} and {solution_end}.\"\"\"\n",
        "\n",
        "TEMPLATE = \"\"\"<start_of_turn>user\n",
        "{system_prompt}\n",
        "\n",
        "{question}<end_of_turn>\n",
        "<start_of_turn>model\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WASP9N5JTn1k",
      "metadata": {},
      "source": [
        "We use OpenAI's GSM8K dataset. GSM8K comprises grade school math word problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gTGjcSMNTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_hash_answer(text: str) -> str | None:\n",
        "  if \"####\" not in text:\n",
        "    return None\n",
        "  return text.split(\"####\")[1].strip()\n",
        "\n",
        "\n",
        "def get_dataset(data_dir, split=\"train\") -> grain.MapDataset:\n",
        "  # Download data\n",
        "  if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "  data = tfds.data_source(\n",
        "      \"gsm8k\",\n",
        "      split=split,\n",
        "      data_dir=data_dir,\n",
        "      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
        "      download=True,\n",
        "  )\n",
        "\n",
        "  dataset = (\n",
        "      grain.MapDataset.source(data)\n",
        "      .shuffle(seed=42)\n",
        "      .map(\n",
        "          lambda x: {\n",
        "              # passed to model forward pass\n",
        "              \"prompts\": TEMPLATE.format(\n",
        "                  system_prompt=SYSTEM_PROMPT,\n",
        "                  question=x[\"question\"].decode(\"utf-8\"),\n",
        "              ),\n",
        "              # passed to reward functions\n",
        "              \"question\": x[\"question\"].decode(\"utf-8\"),\n",
        "              # passed to reward functions\n",
        "              \"answer\": extract_hash_answer(x[\"answer\"].decode(\"utf-8\")),\n",
        "          }\n",
        "      )\n",
        "  )\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KXhOL6GyTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = get_dataset(TRAIN_DATA_DIR, \"train\").batch(BATCH_SIZE)[:NUM_BATCHES]\n",
        "\n",
        "if TRAIN_FRACTION == 1.0:\n",
        "  train_dataset = dataset.repeat(NUM_EPOCHS)\n",
        "  val_dataset = None\n",
        "else:\n",
        "  train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n",
        "  train_dataset = train_dataset.repeat(NUM_EPOCHS)\n",
        "\n",
        "  val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n",
        "\n",
        "test_dataset = get_dataset(TEST_DATA_DIR, \"test\").batch(BATCH_SIZE)[\n",
        "    :NUM_TEST_BATCHES\n",
        "]\n",
        "\n",
        "len(train_dataset), len(val_dataset) if val_dataset is not None else 0, len(\n",
        "    test_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k7n8L0VzTn1k",
      "metadata": {},
      "source": [
        "Let's see how one batch of the dataset looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5TF-wNQ2Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for ele in train_dataset[:1]:\n",
        "#   pprint(ele)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BZxBR7Y_Tn1k",
      "metadata": {},
      "source": [
        "## Load the policy model and the reference model\n",
        "\n",
        "The policy model is the model which is actually trained and whose weights are\n",
        "updated. The reference model is the model with which we compute KL divergence.\n",
        "This is to ensure that the policy updates are not huge and that it does not\n",
        "deviate too much from the reference model.\n",
        "\n",
        "Typically, the reference model is the base model, and the policy model is the\n",
        "same base model, but with LoRA parameters. Only the LoRA parameters are updated.\n",
        "\n",
        "Note: We perform full precision (fp32) training. You can, however, leverage\n",
        "Qwix for QAT.\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
        "to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.com/models/google/gemma/flax/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thp6hhqfTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Log in\n",
        "# if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "#   kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "srH2s_jzTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# kaggle_ckpt_path = kagglehub.model_download(\"google/gemma-2/flax/gemma2-2b-it\")\n",
        "# kaggle_ckpt_path = kagglehub.model_download(\"google/gemma/flax/2b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cIFAxgVOTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # This is a workaround. The checkpoints on Kaggle don't work with NNX. So, we\n",
        "# # load the model, save the checkpoint locally, and then reload the model\n",
        "# # (sharded).\n",
        "# params = params_lib.load_and_format_params(\n",
        "#     os.path.join(kaggle_ckpt_path, \"gemma2-2b-it\")\n",
        "# )\n",
        "# gemma = gemma_lib.Transformer.from_params(params, version=\"2-2b-it\")\n",
        "# checkpointer = ocp.StandardCheckpointer()\n",
        "# _, state = nnx.split(gemma)\n",
        "# checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JSz-XmQpTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Wait for the ckpt to save successfully.\n",
        "# time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_w8kav8sTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Delete the intermediate model to save memory.\n",
        "# del params\n",
        "# del gemma\n",
        "# del state\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "968a4626",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tunix.rl import utils\n",
        "# show_hbm_usage = utils.show_hbm_usage\n",
        "\n",
        "print(\"HBM usage before loading model:\")\n",
        "show_hbm_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b18135",
      "metadata": {},
      "source": [
        "### Load MaxText model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6aa758c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# add the parent directory (one level up) to sys.path\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../../maxtext')))\n",
        "\n",
        "# ! pip install -r ../../maxtext/requirements.txt\n",
        "\n",
        "import MaxText as mt\n",
        "from MaxText import pyconfig\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eee062b3",
      "metadata": {},
      "source": [
        "#### Convert MaxText model to nnx (use a commit from MaxText repo prior to )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m2KD-nmbTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from MaxText.integrations.tunix.tunix_utils import build_tunix_wrapper\n",
        "from flax import linen as nn\n",
        "from tunix.models.llama3 import model as llama3_lib\n",
        "from functools import partial\n",
        "\n",
        "from etils import epath\n",
        "\n",
        "\n",
        "def get_ref_maxtext_model(config):\n",
        "\n",
        "  def create_model(config):\n",
        "    return mt.from_pretrained(config, rngs=nnx.Rngs(params=0, dropout=1))\n",
        "\n",
        "  abstract_model = nnx.eval_shape(create_model, config=config)\n",
        "  graphdef, abstract_state = nnx.split(abstract_model)\n",
        "  print(\"The abstract NNX state (all leaves are abstract arrays):\")\n",
        "  nnx.display(abstract_state)\n",
        "  specs = nnx.get_partition_spec(abstract_state)\n",
        "  mesh = abstract_model.mesh\n",
        "\n",
        "  # JIT a function that creates the model state with proper sharding from the start.\n",
        "  # By providing out_shardings, we instruct JAX to produce sharded output directly,\n",
        "  # avoiding a large intermediate allocation on a single device.\n",
        "  with nn.logical_axis_rules(config.logical_axis_rules):\n",
        "    out_shardings = nn.logical_to_mesh_sharding(specs, mesh)\n",
        "\n",
        "  @functools.partial(jax.jit, out_shardings=out_shardings)\n",
        "  def create_sharded_state():\n",
        "    # This will be JIT-compiled. JAX knows the output sharding and can\n",
        "    # initialize the parameters directly on the target devices in a sharded way.\n",
        "    model = create_model(config)\n",
        "    return nnx.state(model)\n",
        "\n",
        "  with mesh:\n",
        "    # Create the model with sharded parameters.\n",
        "    sharded_state = create_sharded_state()\n",
        "    model = nnx.merge(graphdef, sharded_state)\n",
        "\n",
        "    if config.load_parameters_path:\n",
        "      target_for_restore = jax.tree.map(\n",
        "          lambda v: v.value,\n",
        "          sharded_state,\n",
        "          is_leaf=lambda n: isinstance(n, nnx.Variable),\n",
        "      )\n",
        "\n",
        "      try:\n",
        "        ckptr = ocp.Checkpointer(\n",
        "            ocp.PyTreeCheckpointHandler(\n",
        "                restore_concurrent_gb=None,\n",
        "                save_concurrent_gb=None,\n",
        "                use_ocdbt=True,\n",
        "                use_zarr3=True,\n",
        "            )\n",
        "        )\n",
        "        # This is a memory optimization. We don't want to restore the entire checkpoint - only the params.\n",
        "        # Rather than pass the entire abstract state, which could unnecessarily restore opt_state and such and waste\n",
        "        # memory, we instead specify here that we are just restoring the params field of the checkpoint\n",
        "        # (which itself may be a dictionary containing a key named 'params').\n",
        "        restore_args = ocp.checkpoint_utils.construct_restore_args(\n",
        "            target_for_restore\n",
        "        )\n",
        "        restored = ckptr.restore(\n",
        "            epath.Path(config.load_parameters_path),\n",
        "            item={\"params\": {\"params\": target_for_restore}},\n",
        "            transforms={},\n",
        "            restore_args={\"params\": {\"params\": restore_args}},\n",
        "        )\n",
        "        checkpoint = restored[\"params\"][\"params\"]\n",
        "\n",
        "        if checkpoint:\n",
        "          nnx.update(model, checkpoint)\n",
        "\n",
        "      except Exception as e:\n",
        "        raise ValueError(f\"Checkpointing failed: {e}\")\n",
        "\n",
        "    tunix_model = TunixMaxTextLlama(\n",
        "        base_model=model,\n",
        "        use_attention_mask=False,  # trust Tunix loss masking\n",
        "    )\n",
        "\n",
        "    model_config = llama3_lib.ModelConfig.llama3_1_8b()\n",
        "    tunix_model.config = model_config\n",
        "\n",
        "  return tunix_model, mesh, model_config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8efa8c5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_maxtext_to_vllm_mappings():\n",
        "  \"\"\"Create mappings for transferring MaxText scanned state to vLLM unscanned state.\"\"\"\n",
        "  return {\n",
        "      # Token embeddings - shard vocab dimension for TP\n",
        "      'base.token_embedder.embedding': (\n",
        "          'model.embed.embedding',\n",
        "          ('model', None),\n",
        "      ),  # checked\n",
        "      # Final layer norm - no sharding needed\n",
        "      'base.decoder.decoder_norm.scale': (\n",
        "          'model.norm.scale',\n",
        "          (None,),\n",
        "      ),  # checked\n",
        "      # LM head (logits projection) - shard vocab dimension for TP\n",
        "      'base.decoder.logits_dense.kernel': (\n",
        "          'model.lm_head',\n",
        "          (None, 'model'),\n",
        "      ),  # checked\n",
        "      # Layer-specific mappings (scanned -> unscanned)\n",
        "      # MLP components - shard hidden dimensions for TP\n",
        "      'base.decoder.layers.mlp.wi_0.kernel': (  # checked\n",
        "          'model.layers.*.mlp.gate_proj.kernel',\n",
        "          (None, 'layer', 'model'),\n",
        "      ),  # gate_proj: (4096, 14336) - shard output\n",
        "      'base.decoder.layers.mlp.wi_1.kernel': (  # checked\n",
        "          'model.layers.*.mlp.up_proj.kernel',\n",
        "          (None, 'layer', 'model'),\n",
        "      ),  # up_proj: (4096, 14336) - shard output\n",
        "      'base.decoder.layers.mlp.wo.kernel': (  # checked\n",
        "          'model.layers.*.mlp.down_proj.kernel',\n",
        "          ('model', 'layer', None),\n",
        "      ),  # down_proj: (14336, 4096) - shard input\n",
        "      # Layer norms - no sharding needed\n",
        "      'base.decoder.layers.pre_self_attention_layer_norm.scale': (\n",
        "          'model.layers.*.input_layernorm.scale',\n",
        "          (None, 'layer'),  # checked\n",
        "      ),\n",
        "      'base.decoder.layers.post_self_attention_layer_norm.scale': (\n",
        "          'model.layers.*.post_attention_layernorm.scale',\n",
        "          (None, 'layer'),  # checked\n",
        "      ),\n",
        "      # Attention components - shard head dimensions for TP\n",
        "      'base.decoder.layers.self_attention.query.kernel': (\n",
        "          'model.layers.*.self_attn.q_proj.kernel',\n",
        "          (None, 'layer', 'model', None),\n",
        "      ),  # q_proj: shard num_heads # NOT MATCH\n",
        "      'base.decoder.layers.self_attention.key.kernel': (\n",
        "          'model.layers.*.self_attn.k_proj.kernel',\n",
        "          (None, 'layer', 'model', None),\n",
        "      ),  # k_proj: shard num_kv_heads\n",
        "      'base.decoder.layers.self_attention.value.kernel': (\n",
        "          'model.layers.*.self_attn.v_proj.kernel',\n",
        "          (None, 'layer', 'model', None),  # match\n",
        "      ),  # v_proj: shard num_kv_heads\n",
        "      'base.decoder.layers.self_attention.out.kernel': (\n",
        "          'model.layers.*.self_attn.o_proj.kernel',\n",
        "          ('model', 'layer', None, None),\n",
        "      ),  # o_proj: shard input heads #match\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76dcf523",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "transpose_keys = {}\n",
        "\n",
        "\n",
        "def reorder_rope(arr):\n",
        "  evens = arr[..., ::2]\n",
        "  odds = arr[..., 1::2]\n",
        "  return jax.numpy.concatenate((evens, odds), axis=arr.ndim - 1)\n",
        "\n",
        "\n",
        "def transform_query_kernel(arr):\n",
        "  head_dim = arr.shape[-1]\n",
        "  assert head_dim == 128  # hard coded for now\n",
        "  depth_scale = np.dtype('float32').type(np.sqrt(head_dim))\n",
        "  arr = arr * depth_scale\n",
        "  return reorder_rope(arr)\n",
        "\n",
        "\n",
        "def transform_key_kernel(arr):\n",
        "  return reorder_rope(arr)\n",
        "\n",
        "\n",
        "hook_fns = {\n",
        "    'base.decoder.layers.self_attention.query.kernel': transform_query_kernel,\n",
        "    'base.decoder.layers.self_attention.key.kernel': transform_key_kernel,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kSdZ7aGhTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base model\n",
        "# gemma, mesh, model_config = get_base_model(\n",
        "#     ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
        "# )\n",
        "from MaxText.integration.tunix.tunix_adaptor import TunixMaxTextLlama\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()  # To fix \"This event loop is already running\" error in Colab\n",
        "config_ref = pyconfig.initialize(\n",
        "    [\n",
        "        \"\",\n",
        "        \"../../maxtext/MaxText/configs/base.yml\",\n",
        "    ],  # TODO: @mazumdera: why decode.py?\n",
        "    base_output_directory=\"gs://dummy_output_dir\",  # This is not used in Tunix.\n",
        "    run_name=\"test-tunix-maxtext-llama3.1-8b\",\n",
        "    tokenizer_type=\"tiktoken\",\n",
        "    tokenizer_path=\"assets/tokenizer_llama3.tiktoken\",\n",
        "    load_parameters_path=\"gs://maxtext-model-checkpoints/llama3.1-8b/2025-01-23-19-04/scanned/0/items\",\n",
        "    per_device_batch_size=1,\n",
        "    max_prefill_predict_length=4,\n",
        "    max_target_length=1024,\n",
        "    steps=10,\n",
        "    async_checkpointing=\"false\",\n",
        "    model_name=\"llama3.1-8b\",\n",
        "    checkpoint_period=5,\n",
        "    skip_jax_distributed_system=\"true\",\n",
        "    weight_dtype=\"bfloat16\",\n",
        "    attention=\"dot_product\",\n",
        "    remat_policy=\"none\",\n",
        "    # decoder_layer_input=\"offload\",\n",
        "    # query_proj=\"offload\",\n",
        "    # key_proj=\"offload\",\n",
        "    # value_proj=\"offload\",\n",
        "    opt_type=\"sgd\",\n",
        ")\n",
        "\n",
        "llama3_1_8b, mesh, model_config = get_ref_maxtext_model(config_ref)\n",
        "# gemma_maxtext_nnx = nnx.bridge.ToNNX(gemma)\n",
        "# Instead of:\n",
        "nnx.display(llama3_1_8b)\n",
        "\n",
        "\n",
        "# Use:\n",
        "print(\"Model initialized successfully\")\n",
        "print(f\"Model mesh shape: {mesh.shape}\")\n",
        "print(f\"Model config: {model_config}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f04f51f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "_maxtext_state_flatten = nnx.state(llama3_1_8b).flat_state()\n",
        "maxtext_state_flatten = {\n",
        "    '.'.join(str(key) for key in keys): v for keys, v in _maxtext_state_flatten\n",
        "}\n",
        "print(f\"maxtext_state_flatten[base.token_embedder.embedding].value={maxtext_state_flatten['base.token_embedder.embedding'].value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4200ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"HBM usage after loading ref model:\")\n",
        "show_hbm_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f04a92e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# # Policy model\n",
        "# lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "# nnx.display(lora_gemma)\n",
        "\n",
        "\n",
        "# Policy model\n",
        "# This can remain unchanged from default Tunix's colab\n",
        "# lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "\n",
        "# TODO: @mazumdera: change this to use lora\n",
        "# lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "# nnx.display(lora_gemma)\n",
        "\n",
        "config_policy = pyconfig.initialize(\n",
        "      [\"\", \"/home/linchai_google_com/maxtext/MaxText/configs/base.yml\"], #TODO: @mazumdera: why decode.py?\n",
        "      base_output_directory=\"gs://dummy_output_dir\",  # This is not used in Tunix.\n",
        "      run_name=\"test-tunix-maxtext-llama3.1-8b\",\n",
        "      # run_name=\"test-tunix-maxtext-llama3.1-8b\",\n",
        "      # dataset_path=we use Tunix's dataset\n",
        "      #TODO: @mazumdera: change this to use checkpoint\n",
        "      tokenizer_type=\"tiktoken\",\n",
        "      tokenizer_path=\"assets/tokenizer_llama3.tiktoken\",\n",
        "      load_parameters_path=\"gs://maxtext-model-checkpoints/llama3.1-8b/2025-01-23-19-04/scanned/0/items\",\n",
        "      # tokenizer_path=\"assets/tokenizer.gemma\",\n",
        "      per_device_batch_size=1,\n",
        "      max_prefill_predict_length=4,\n",
        "      max_target_length=1024,\n",
        "      steps=10,\n",
        "      async_checkpointing=\"false\",\n",
        "      model_name=\"llama3.1-8b\",\n",
        "      # model_name=\"gemma-2b\",\n",
        "      checkpoint_period=5,\n",
        "      skip_jax_distributed_system=\"true\",\n",
        "      weight_dtype=\"bfloat16\",\n",
        "      attention=\"dot_product\",\n",
        "      remat_policy=\"none\",\n",
        "      # decoder_layer_input=\"offload\",\n",
        "      # query_proj=\"offload\",\n",
        "      # key_proj=\"offload\",\n",
        "      # value_proj=\"offload\",\n",
        "      opt_type=\"sgd\",\n",
        "  )\n",
        "llama3_1_8b_policy, mesh_policy, model_config_policy = get_ref_maxtext_model(config_policy)\n",
        "\n",
        "\n",
        "llama3_1_8b_policy.to_hf_mappings = create_maxtext_to_vllm_mappings\n",
        "llama3_1_8b_policy.to_hf_transpose_keys = lambda *args: transpose_keys\n",
        "llama3_1_8b_policy.lora_to_hf_mappings = lambda *args: None  # No LoRA\n",
        "llama3_1_8b_policy.to_hf_hook_fns = lambda *args: hook_fns\n",
        "\n",
        "# gemma_maxtext_nnx = nnx.bridge.ToNNX(gemma)\n",
        "# Instead of:\n",
        "nnx.display(llama3_1_8b_policy)\n",
        "\n",
        "# Use:\n",
        "print(\"Model initialized successfully\")\n",
        "print(f\"Model mesh shape: {mesh_policy.shape}\")\n",
        "print(f\"Model config: {model_config_policy}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9801570c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"HBM usage after loading policy model:\")\n",
        "show_hbm_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd374819",
      "metadata": {},
      "outputs": [],
      "source": [
        "_maxtext_state_flatten = nnx.state(llama3_1_8b_policy).flat_state()\n",
        "maxtext_state_flatten = {\n",
        "    '.'.join(str(key) for key in keys): v for keys, v in _maxtext_state_flatten\n",
        "}\n",
        "print(f\"maxtext_state_flatten[base.token_embedder.embedding].value={maxtext_state_flatten['base.token_embedder.embedding'].value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zLzR1tJfTn1k",
      "metadata": {},
      "source": [
        "## Define reward functions\n",
        "\n",
        "We define four reward functions:\n",
        "\n",
        "- reward if the format of the output exactly matches the instruction given in\n",
        "`TEMPLATE`;\n",
        "- reward if the format of the output approximately matches the instruction given\n",
        "in `TEMPLATE`;\n",
        "- reward if the answer is correct/partially correct;\n",
        "- Sometimes, the text between `<answer>`, `</answer>` might not be one\n",
        "  number. So, extract the number, and reward the model if the answer is correct.\n",
        "\n",
        "The reward functions are inspired from\n",
        "[here](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb).\n",
        "\n",
        "First off, let's define a RegEx for checking whether the format matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C7Beft8wTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "match_format = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\n",
        "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
        "    rf\"{solution_start}(.+?){solution_end}\"\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags=re.MULTILINE | re.DOTALL,\n",
        ")\n",
        "\n",
        "match_format.search(\n",
        "    f\"{reasoning_start}Let me\"\n",
        "    f\" think!{reasoning_end}{solution_start}2{solution_end}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fe1rF15zTn1k",
      "metadata": {},
      "source": [
        "Give the model a reward of 3 points if the format matches exactly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_fhQ6pY2Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def match_format_exactly(prompts, completions, **kargs):\n",
        "  scores = []\n",
        "  for completion in completions:\n",
        "    score = 0\n",
        "    response = completion\n",
        "    # Match if format is seen exactly!\n",
        "    if match_format.search(response) is not None:\n",
        "      score += 3.0\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sWdAdUHuTn1k",
      "metadata": {},
      "source": [
        "We also reward the model if the format of the output matches partially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uOhO4f3-Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def match_format_approximately(prompts, completions, **kargs):\n",
        "  scores = []\n",
        "\n",
        "  for completion in completions:\n",
        "    score = 0\n",
        "    response = completion\n",
        "    # Count how many keywords are seen - we penalize if too many!\n",
        "    # If we see 1, then plus some points!\n",
        "    score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
        "    score += 0.5 if response.count(reasoning_end) == 1 else -0.5\n",
        "    score += 0.5 if response.count(solution_start) == 1 else -0.5\n",
        "    score += 0.5 if response.count(solution_end) == 1 else -0.5\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A2fNZDgTTn1k",
      "metadata": {},
      "source": [
        "Reward the model if the answer is correct. A reward is also given if the answer\n",
        "does not match exactly, i.e., based on how close the answer is to the correct\n",
        "value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S8zcWsmhTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_answer(prompts, completions, answer, **kargs):\n",
        "  responses = completions\n",
        "\n",
        "  extracted_responses = [\n",
        "      guess.group(1) if (guess := match_format.search(r)) is not None else None\n",
        "      for r in responses\n",
        "  ]\n",
        "\n",
        "  scores = []\n",
        "  for guess, true_answer in zip(extracted_responses, answer):\n",
        "    score = 0\n",
        "    if guess is None:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "    # Correct answer gets 3 points!\n",
        "    if guess == true_answer:\n",
        "      score += 3.0\n",
        "    # Match if spaces are seen\n",
        "    elif guess.strip() == true_answer.strip():\n",
        "      score += 1.5\n",
        "    else:\n",
        "      # We also reward it if the answer is close via ratios!\n",
        "      # Ie if the answer is within some range, reward it!\n",
        "      try:\n",
        "        ratio = float(guess) / float(true_answer)\n",
        "        if ratio >= 0.9 and ratio <= 1.1:\n",
        "          score += 0.5\n",
        "        elif ratio >= 0.8 and ratio <= 1.2:\n",
        "          score += 0.25\n",
        "        else:\n",
        "          score -= 1.0  # Penalize wrong answers\n",
        "      except:\n",
        "        score -= 0.5  # Penalize\n",
        "    scores.append(score)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nIpOVv78Tn1k",
      "metadata": {},
      "source": [
        "Sometimes, the text between `<answer>` and `</answer>` might not be one\n",
        "number; it can be a sentence. So, we extract the number and compare the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NXvRtbk8Tn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n",
        ")\n",
        "match_numbers.findall(f\"{solution_start}  0.34  {solution_end}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oxZQAFKOTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_numbers(prompts, completions, answer, **kargs):\n",
        "  question = kargs[\"question\"]\n",
        "  responses = completions\n",
        "\n",
        "  extracted_responses = [\n",
        "      guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n",
        "      for r in responses\n",
        "  ]\n",
        "\n",
        "  scores = []\n",
        "  print(\"START ============================\")\n",
        "  print(f\"Question: {question[0]}\")\n",
        "  print(f\"Answer: {answer[0]}\")\n",
        "  print(f\"Response: {responses[0]}\")\n",
        "  print(f\"Extracted: {extracted_responses[0]}\")\n",
        "  print(\"END ==============================\")\n",
        "  for guess, true_answer in zip(extracted_responses, answer):\n",
        "    if guess is None:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "    # Convert to numbers\n",
        "    try:\n",
        "      true_answer = float(true_answer.strip())\n",
        "      guess = float(guess.strip())\n",
        "      scores.append(1.5 if guess == true_answer else 0.0)\n",
        "    except:\n",
        "      scores.append(0)\n",
        "      continue\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AaiYMJxFTn1k",
      "metadata": {},
      "source": [
        "## Evaluate\n",
        "\n",
        "\n",
        "Before we train the model, let's evaluate the model on the test set so we can\n",
        "see the improvement post training.\n",
        "\n",
        "We evaluate it in two ways:\n",
        "\n",
        "**Quantitative**\n",
        "\n",
        "* **Answer Accuracy**: percentage of samples for which the model predicts the\n",
        "correct final numerical answer  \n",
        "* **Answer (Partial) Accuracy**: percentage of samples for which the model\n",
        "predicts a final numerical answer such that the \\`model answer / answer\\`\n",
        "ratio lies between 0.9 and 1.1.  \n",
        "* **Format Accuracy**: percentage of samples for which the model outputs the\n",
        "correct format, i.e., reasoning between the reasoning special tokens, and the\n",
        "final answer between the \\`\\<start\\_answer\\>\\`, \\`\\<end\\_answer\\>\\` tokens.\n",
        "\n",
        "**Qualitative**\n",
        "\n",
        "We'll also print outputs for a few given questions so that we can compare the generated output later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_k58bOicUHJy",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def generate(\n",
        "#     question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n",
        "# ):\n",
        "#   \"\"\"Given prompt, generates text.\"\"\"\n",
        "\n",
        "#   if isinstance(question, str):\n",
        "#     input_batch = [\n",
        "#         TEMPLATE.format(\n",
        "#             system_prompt=SYSTEM_PROMPT,\n",
        "#             question=question,\n",
        "#         ),\n",
        "#     ]\n",
        "#   else:\n",
        "#     input_batch = [\n",
        "#         TEMPLATE.format(\n",
        "#             system_prompt=SYSTEM_PROMPT,\n",
        "#             question=q,\n",
        "#         )\n",
        "#         for q in question\n",
        "#     ]\n",
        "\n",
        "#   out_data = sampler(\n",
        "#       input_strings=input_batch,\n",
        "#       total_generation_steps=768,\n",
        "#       temperature=temperature,\n",
        "#       top_k=top_k,\n",
        "#       top_p=top_p,\n",
        "#       echo=False,\n",
        "#       seed=seed if seed is not None else None,\n",
        "#   )\n",
        "\n",
        "#   output = out_data.text\n",
        "#   if isinstance(question, str):\n",
        "#     return output[0]\n",
        "#   return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yJo2nuKB-wlw",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def evaluate(\n",
        "#     dataset,\n",
        "#     sampler,\n",
        "#     temperature=0.7,\n",
        "#     top_k=50,\n",
        "#     top_p=0.95,\n",
        "#     num_passes=1,\n",
        "#     corr_lst=False,\n",
        "#     make_lst=False,\n",
        "# ):\n",
        "#   \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n",
        "\n",
        "#   response_lst = []\n",
        "#   corr = 0\n",
        "#   partially_corr = 0\n",
        "#   corr_format = 0\n",
        "#   total = 0\n",
        "\n",
        "#   for batch in tqdm(dataset):\n",
        "#     answers = batch[\"answer\"]\n",
        "#     questions = batch[\"question\"]\n",
        "\n",
        "#     multiple_call_responses = [[] for _ in range(len(questions))]\n",
        "#     for p in range(num_passes):\n",
        "#       responses = generate(\n",
        "#           questions, sampler, temperature, top_k, top_p, seed=p\n",
        "#       )\n",
        "#       for idx, response in enumerate(responses):\n",
        "#         multiple_call_responses[idx].append(response)\n",
        "\n",
        "#     for question, multiple_call_response, answer in zip(\n",
        "#         questions, multiple_call_responses, answers\n",
        "#     ):\n",
        "#       # check answer\n",
        "#       corr_ctr_per_question = 0\n",
        "#       partially_corr_per_question = 0\n",
        "#       corr_format_per_question = 0\n",
        "#       for response in multiple_call_response:\n",
        "#         extracted_response = (\n",
        "#             guess.group(1)\n",
        "#             if (guess := match_numbers.search(response)) is not None\n",
        "#             else \"-1000000\"\n",
        "#         )\n",
        "#         try:\n",
        "#           if float(extracted_response.strip()) == float(answer.strip()):\n",
        "#             corr_ctr_per_question += 1\n",
        "\n",
        "#           ratio = float(extracted_response.strip()) / float(answer.strip())\n",
        "#           if ratio >= 0.9 and ratio <= 1.1:\n",
        "#             partially_corr_per_question += 1\n",
        "#         except:\n",
        "#           print(\"SKIPPED\")\n",
        "\n",
        "#         # check format\n",
        "#         if match_format.search(response) is not None:\n",
        "#           corr_format_per_question += 1\n",
        "\n",
        "#         if (\n",
        "#             corr_ctr_per_question > 0\n",
        "#             and partially_corr_per_question > 0\n",
        "#             and corr_format_per_question > 0\n",
        "#         ):\n",
        "#           break\n",
        "\n",
        "#       if corr_ctr_per_question > 0:\n",
        "#         corr += 1\n",
        "#         if corr_lst and make_lst:\n",
        "#           response_lst.append((question, answer, multiple_call_response))\n",
        "#       else:\n",
        "#         if not corr_lst and make_lst:\n",
        "#           response_lst.append((question, answer, multiple_call_response))\n",
        "#       if partially_corr_per_question > 0:\n",
        "#         partially_corr += 1\n",
        "#       if corr_format_per_question > 0:\n",
        "#         corr_format += 1\n",
        "\n",
        "#       total += 1\n",
        "#       if total % 10 == 0:\n",
        "#         print(\n",
        "#             f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n",
        "#             f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n",
        "#         )\n",
        "\n",
        "#   to_return = (\n",
        "#       corr,\n",
        "#       total,\n",
        "#       corr / total * 100,\n",
        "#       partially_corr / total * 100,\n",
        "#       corr_format / total * 100,\n",
        "#   )\n",
        "#   if make_lst:\n",
        "#     return to_return, response_lst\n",
        "#   return to_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HZMO-KflTn1k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# gemma_tokenizer = data_lib.GemmaTokenizer()\n",
        "# sampler = sampler_lib.Sampler(\n",
        "#     # transformer=lora_gemma,\n",
        "#     transformer=llama3_1_8b_policy,\n",
        "#     tokenizer=gemma_tokenizer,\n",
        "#     cache_config=sampler_lib.CacheConfig(\n",
        "#         cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "#         num_layers=model_config.num_layers,\n",
        "#         num_kv_heads=model_config.num_kv_heads,\n",
        "#         head_dim=model_config.head_dim,\n",
        "#     ),\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YQM-tzXWUmoE",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
        "#     test_dataset,\n",
        "#     sampler,\n",
        "#     **GENERATION_CONFIGS[\"greedy\"],\n",
        "# )\n",
        "# print(\n",
        "#     f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
        "#     f\" {format_accuracy=}%\"\n",
        "# )\n",
        "\n",
        "# # TODO: @mazumdera: why is this 0?\n",
        "# # corr=0, total=5, accuracy=0.0%, partial_accuracy=0.0%, format_accuracy=0.0%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PJV6wNGY-3PG",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for eval_example in QUALITATIVE_EVAL_EXAMPLES:\n",
        "#   question = eval_example[\"question\"]\n",
        "#   answer = eval_example[\"answer\"]\n",
        "#   response = generate(\n",
        "#       question,\n",
        "#       sampler,\n",
        "#       temperature=INFERENCE_TEMPERATURE,\n",
        "#       top_k=INFERENCE_TOP_K,\n",
        "#       top_p=INFERENCE_TOP_P,\n",
        "#   )\n",
        "\n",
        "#   print(f\"Question:\\n{question}\")\n",
        "#   print(f\"Answer:\\n{answer}\")\n",
        "#   print(f\"Response:\\n{response}\")\n",
        "#   print(\"===============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-CmB2ZT9Tn1l",
      "metadata": {},
      "source": [
        "## Train\n",
        "\n",
        "Let's set up all the configs first - checkpointing, metric logging and training.\n",
        "We then train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mHzdsYsGTn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ckpt saving\n",
        "checkpointing_options = ocp.CheckpointManagerOptions(\n",
        "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
        ")\n",
        "\n",
        "# Metrics logger\n",
        "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/home/linchai_google_com/content/tmp/tensorboard/grpo\", flush_every_n_steps=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u1Sc1fNC_CJ7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logs\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /home/linchai_google_com/content/tmp/tensorboard/grpo --port=0\n",
        "%reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_6VxFW1ZTn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Training config\n",
        "# training_config = GrpoTrainingConfig(\n",
        "#     max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "#     total_generation_steps=TOTAL_GENERATION_STEPS,\n",
        "#     num_generations=NUM_GENERATIONS,\n",
        "#     num_iterations=NUM_ITERATIONS,\n",
        "#     beta=BETA,\n",
        "#     epsilon=EPSILON,\n",
        "#     temperature=TEMPERATURE,\n",
        "#     top_p=TOP_P,\n",
        "#     top_k=TOP_K,\n",
        "#     eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "#     max_steps=MAX_STEPS,\n",
        "#     # metrics logging\n",
        "#     metrics_logging_options=metrics_logging_options,\n",
        "#     # checkpoint saving\n",
        "#     checkpoint_root_directory=CKPT_DIR,\n",
        "#     checkpointing_options=checkpointing_options,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OIe1lO08Tn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer, learning rate scheduler, gradient clipping\n",
        "# optimizer = optax.adamw(\n",
        "#     learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "#         init_value=0.0,\n",
        "#         peak_value=LEARNING_RATE,\n",
        "#         warmup_steps=WARMUP_STEPS,\n",
        "#         decay_steps=MAX_STEPS,\n",
        "#         end_value=0.0,\n",
        "#     ),\n",
        "#     b1=B1,\n",
        "#     b2=B2,\n",
        "#     weight_decay=WEIGHT_DECAY,\n",
        "# )\n",
        "#TODO: @mazumdera: try optimizer offloading with adamw\n",
        "optimizer = optax.adafactor(\n",
        "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=LEARNING_RATE,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        decay_steps=MAX_STEPS,\n",
        "        end_value=0.0,\n",
        "    ),\n",
        ")\n",
        "if MAX_GRAD_NORM is not None:\n",
        "  optimizer = optax.chain(\n",
        "      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
        "      optimizer,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_6VxFW1ZTn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training config\n",
        "ROLLOUT_MESH = [(1, 8), (\"fsdp\", \"tp\")]   # simpler mesh for rollout\n",
        "rollout_mesh = jax.make_mesh(*MESH, devices=jax.devices())\n",
        "cluster_config = rl_cluster_lib.ClusterConfig(\n",
        "    role_to_mesh={\n",
        "        rl_cluster_lib.Role.ACTOR: mesh,\n",
        "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
        "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
        "    },\n",
        "    rollout_engine='vllm',\n",
        "    offload_to_cpu=False,\n",
        "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
        "        actor_optimizer=optimizer,\n",
        "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "        max_steps=MAX_STEPS,\n",
        "        gradient_accumulation_steps=1,\n",
        "        # metrics logging\n",
        "        metrics_logging_options=metrics_logging_options,\n",
        "        # checkpoint saving\n",
        "        checkpoint_root_directory=CKPT_DIR,\n",
        "        checkpointing_options=checkpointing_options,\n",
        "    ),\n",
        "    rollout_config=base_rollout.RolloutConfig(\n",
        "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
        "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        top_k=TOP_K,\n",
        "    ),\n",
        "    rollout_vllm_model_version=\"meta-llama/Meta-Llama-3.1-8B\",\n",
        "    rollout_vllm_hbm_utilization=0.2,\n",
        "    rollout_vllm_tpu_backend_type=\"jax\",\n",
        "    # rollout_vllm_init_with_random_weights=True,\n",
        ")\n",
        "\n",
        "grpo_config = GrpoConfig(\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    num_iterations=NUM_ITERATIONS,\n",
        "    beta=BETA,\n",
        "    epsilon=EPSILON,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edeeb04f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # test trainer logic only to see HBM usage\n",
        "# show_hbm_usage()\n",
        "# actor_trainer = rl_cluster_lib.rl_trainer.Trainer(\n",
        "#     model=llama3_1_8b_policy,\n",
        "#     optimizer=cluster_config.training_config.actor_optimizer,\n",
        "#     training_config=cluster_config.training_config)\n",
        "\n",
        "# import numpy as np\n",
        "# from tunix.sft import peft_trainer\n",
        "# def dummy_datasets(batch_size: int, repeat: int = 1):\n",
        "#   # (num_batch, batch_size, seq_len)\n",
        "#   dummy_input = np.arange(1024).reshape((-1, batch_size, 256))\n",
        "#   print(f\"dummy_input.shape={dummy_input.shape}\")\n",
        "#   return [\n",
        "#       peft_trainer.TrainingInput(\n",
        "#           input_tokens=x, input_mask=jnp.ones(x.shape, dtype=jnp.int32)\n",
        "#       )\n",
        "#       for x in dummy_input\n",
        "#   ] * repeat\n",
        "\n",
        "# def dummy_gen_model_input_fn(x: peft_trainer.TrainingInput):\n",
        "#   return {\n",
        "#       'input_tokens': x.input_tokens,\n",
        "#       'input_mask': x.input_mask,\n",
        "#       'positions': jnp.arange(x.input_tokens.shape[1]),\n",
        "#       'attention_mask': jnp.ones_like(x.input_tokens),\n",
        "#   }\n",
        "\n",
        "# actor_trainer.with_gen_model_input_fn(dummy_gen_model_input_fn)\n",
        "# import jax\n",
        "# jax.profiler.start_trace(\"gs://linchai-bucket/maxtext_tpu_vllm_xprof/grpo\")\n",
        "# with mesh:\n",
        "#     actor_trainer.train(dummy_datasets(1, 4))\n",
        "# jax.profiler.stop_trace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2517d77",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Now lora_gemma's parameters are annotated with the specified sharding.\n",
        "# When lora_gemma is used inside a jitted function, JAX will respect these\n",
        "# shardings.\n",
        "\n",
        "# You can inspect the sharding of a parameter's value.\n",
        "# The sharding will be concrete after being passed through a jitted function.\n",
        "# @jax.jit\n",
        "# def get_sharded_kernel(model):\n",
        "#     return model.base.token_embedder.embedding\n",
        "\n",
        "# with mesh:\n",
        "#     sharded_kernel_value = get_sharded_kernel(llama3_1_8b_policy)\n",
        "\n",
        "# print(\"Sharding of embed kernel:\")\n",
        "# print(sharded_kernel_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef184e65",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RL cluster\n",
        "\n",
        "rl_cluster = rl_cluster_lib.RLCluster(\n",
        "    actor=llama3_1_8b_policy,\n",
        "    reference=llama3_1_8b,\n",
        "    tokenizer=model_tokenizer,\n",
        "    cluster_config=cluster_config,\n",
        ")\n",
        "\n",
        "# GRPO Trainer\n",
        "grpo_trainer = GrpoLearner(\n",
        "    rl_cluster=rl_cluster,\n",
        "    reward_fns=[\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    grpo_config=grpo_config,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40cb7045",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # verify if vllm sampler works\n",
        "# from tunix.rl.rollout.base_rollout import RolloutConfig\n",
        "# output = rl_cluster.rollout.generate(\n",
        "#     [\"The capital of France is\"],\n",
        "#     rollout_config=RolloutConfig(\n",
        "#         n=1, max_tokens_to_generate=64, temperature=0.1\n",
        "#     ),\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faafd023",
      "metadata": {},
      "outputs": [],
      "source": [
        "# output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a4c8657",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_hbm_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1ec44c",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S27XDebYTn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "jax.profiler.start_trace(\"gs://linchai-bucket/matext_tpu_vllm_xprof/grpo\")\n",
        "with mesh:\n",
        "  grpo_trainer.train(dataset)\n",
        "  show_hbm_usage()\n",
        "jax.profiler.stop_trace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd153b33",
      "metadata": {},
      "outputs": [],
      "source": [
        "show_hbm_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FzIP8glkTn1l",
      "metadata": {},
      "source": [
        "## Evaluate\n",
        "\n",
        "Let's evaluate our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V-73HfP1Tn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Load checkpoint first.\n",
        "\n",
        "# trained_ckpt_path = os.path.join(CKPT_DIR, str(MAX_STEPS), \"model_params\")\n",
        "\n",
        "# abs_params = jax.tree.map(\n",
        "#     lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
        "#     nnx.state(llama3_1_8b_policy, nnx.LoRAParam),\n",
        "# )\n",
        "# checkpointer = ocp.StandardCheckpointer()\n",
        "# trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n",
        "\n",
        "# nnx.update(\n",
        "#     llama3_1_8b_policy,\n",
        "#     jax.tree.map(\n",
        "#         lambda a, b: b,\n",
        "#         nnx.state(llama3_1_8b_policy, nnx.LoRAParam),\n",
        "#         trained_lora_params,\n",
        "#     ),\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1vY9kl-ITn1l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# gemma_tokenizer = data_lib.GemmaTokenizer()\n",
        "# sampler = sampler_lib.Sampler(\n",
        "#     transformer=llama3_1_8b_policy,\n",
        "#     tokenizer=gemma_tokenizer,\n",
        "#     cache_config=sampler_lib.CacheConfig(\n",
        "#         cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "#         num_layers=model_config.num_layers,\n",
        "#         num_kv_heads=model_config.num_kv_heads,\n",
        "#         head_dim=model_config.head_dim,\n",
        "#     ),\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nz0q_gGHqYz6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
        "#     test_dataset,\n",
        "#     sampler,\n",
        "#     **GENERATION_CONFIGS[\"greedy\"],\n",
        "# )\n",
        "# print(\n",
        "#     f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
        "#     f\" {format_accuracy=}%\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jd9gpYVpUd3_",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for eval_example in QUALITATIVE_EVAL_EXAMPLES:\n",
        "#   question = eval_example[\"question\"]\n",
        "#   answer = eval_example[\"answer\"]\n",
        "#   response = generate(\n",
        "#       question,\n",
        "#       sampler,\n",
        "#       temperature=INFERENCE_TEMPERATURE,\n",
        "#       top_k=INFERENCE_TOP_K,\n",
        "#       top_p=INFERENCE_TOP_P,\n",
        "#   )\n",
        "\n",
        "#   print(f\"Question:\\n{question}\")\n",
        "#   print(f\"Answer:\\n{answer}\")\n",
        "#   print(f\"Response:\\n{response}\")\n",
        "#   print(\"===============\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vllm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
