# -*- coding: utf-8 -*-
"""grpo_llama3_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.corp.google.com/drive/1_A5QaGmIvRhGZ0uXx4ZdaexbMz3uDMWz?resourcekey=0-shlFU0Y69RVqrvqhRJnTGQ

<a href="https://colab.research.google.com/github/google/tunix/blob/main/examples/grpo_demo.ipynb" ><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

This tutorial demonstrates training the Gemma 2 2B-IT model on the GSM8K math
reasoning benchmark using Group Relative Policy Optimization (GRPO). GRPO can
enhance your model's problem-solving skills on mathematical word problems,
coding problems, etc.

GRPO is an RL algorithm designed to enhance the reasoning abilities of LLMs. It
is a variant of Proximal Policy Optimization (PPO) that reduces memory usage by
eliminating the need for a separate value function model. GRPO works by
generating multiple responses for a given prompt, evaluating these responses
using a reward model, and then calculating a relative advantage based on the
group's performance to update the policy.

In this tutorial we use Colab's `v2-8` TPU. Let's get started!

## Install necessary libraries
"""

# !pip install -q kagglehub

# # !pip install -q tensorflow
# # !pip install -q tensorboardX
# # !pip install -q grain
# # !pip install --force-reinstall "jax==0.6.2" "jaxlib==0.6.2" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
# !pip install "jax[tpu]==0.6.2" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
# # !pip install -q git+https://github.com/google/tunix
# ! pip install -e ~/tunix/
# !pip install -q git+https://github.com/google/qwix

# !pip uninstall -q -y flax
# !pip install -q git+https://github.com/google/flax.git

# !pip install -q tensorflow-datasets

# !pip install -q git+https://github.com/AI-Hypercomputer/pathways-utils.git

# ! pip install -q ~/tunix/

# !pip install ipywidgets

"""## Imports"""

import functools
import gc
from functools import partial
import os
from pprint import pprint
import re
import time

from flax import nnx
import grain
import humanize
import jax
import jax.numpy as jnp
import kagglehub
import optax
from orbax import checkpoint as ocp
from qwix import lora
from etils import epath
# import tensorflow_datasets as tfds
# from tqdm.auto import tqdm
# from tunix.generate import sampler as sampler_lib
# from tunix.models.gemma import data as data_lib
# from tunix.models.gemma import gemma as gemma_lib
# from tunix.models.gemma import params as params_lib
# from tunix.rl import rl_cluster as rl_cluster_lib
# from tunix.rl.rollout import base_rollout
# from tunix.rl.grpo.grpo_learner import GrpoConfig, GrpoLearner
# from tunix.sft import metrics_logger

os.environ['TPU_LIBRARY_PATH'] = '/home/mazumdera_google_com/venv-py311/lib/python3.11/site-packages/libtpu/libtpu.so'

jax.devices()

"""## Hyperparameters

Let's define the configuration we are going to use. Note that this is by no
means a "perfect" set of hyperparameters. To get good results, you might have
to train the model for longer.
"""

# ====== Data ======
# TRAIN_DATA_DIR = "/home/mazumdera_google_com/tunix/llama3/data/train"
# TEST_DATA_DIR = "/home/mazumdera_google_com/tunix/llama3/data/test"
TRAIN_DATA_DIR = "./data/train_gemma_2b"
TEST_DATA_DIR = "./data/test_gemma_2b"
TRAIN_FRACTION = 1.0

# ====== LoRA ======
RANK = 64
ALPHA = 64.0

# ====== Sharding ======
MESH = [(1, 4), ("fsdp", "tp")]

# ====== GRPO ======
# === Generation during GRPO training ===
MAX_PROMPT_LENGTH = 256
TOTAL_GENERATION_STEPS = 768
# Important to keep a high-ish temperature for varied, diverse responses during
# training.
TEMPERATURE = 0.9
TOP_P = 1.0
TOP_K = 50
# The number of times the policy generates multiple responses for a given prompt
# within a single training step. This corresponds to `G` in Algorithm 1 in the
# paper. The "group" in GRPO comes from here.
NUM_GENERATIONS = 2

# === other GRPO configs ===
# The number of iterations per batch (ùúá in GRPO algo 1).
NUM_ITERATIONS = 1
# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.
# Important to keep a high enough value for this, otherwise, the KL divergence
# can increase unchecked.
BETA = 0.08
# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for
# stable updates.
EPSILON = 0.2

# ====== Training ======
BATCH_SIZE = 1
# Increase `NUM_BATCHES` and `MAX_STEPS` for better results.
NUM_BATCHES = 3738
# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be
# increased to a max. of 330 (if batch size is 4).
NUM_TEST_BATCHES = 5 #100 #Anisha: making it small for quick eval

EVAL_EVERY_N_STEPS = 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.
NUM_EPOCHS = 1  # can potentially train for more epochs

# Number of training steps.
MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)

# === AdamW, warmup, cosine scheduler ===
LEARNING_RATE = 3e-6
B1 = 0.9
B2 = 0.99
WEIGHT_DECAY = 0.1
# == Cosine decay with warmup scheduler ==
# Linearly increase learning rate from 0. to 5e-6 in the first 10% training
# steps, and then gradually decrease the learning rate to 0 using cosine
# scheduler.
WARMUP_STEPS = 0.1 * MAX_STEPS
# == Grad clipping ==
# Grad clipping to prevent large gradients. Found this
# important to keep KL divergence in check.
MAX_GRAD_NORM = 0.1

# Checkpoint saving
# INTERMEDIATE_CKPT_DIR = "/home/mazumdera_google_com/content/intermediate_ckpt_llama3/"
# CKPT_DIR = "/home/mazumdera_google_com/content/ckpts_llama3/"

INTERMEDIATE_CKPT_DIR = "/home/mazumdera_google_com/content/intermediate_ckpt_gemma_2b/"
CKPT_DIR = "/home/mazumdera_google_com/content/ckpts_gemma_2b/"

SAVE_INTERVAL_STEPS = 500
MAX_TO_KEEP = 4

# ====== Inference ======
GENERATION_CONFIGS = {
    # greedy search
    "greedy": {"temperature": 1e-4, "top_k": 1, "top_p": 1.0},
    # some randomness
    "standard": {"temperature": 0.7, "top_k": 50, "top_p": 0.95},
    # liberal
    "liberal": {"temperature": 0.85, "top_k": 2000, "top_p": 1.0},
}

"""## Utility functions"""

def show_hbm_usage():
  """Displays memory usage per device."""
  fmt_size = functools.partial(humanize.naturalsize, binary=True)

  for d in jax.local_devices():
    stats = d.memory_stats()
    used = stats["bytes_in_use"]
    limit = stats["bytes_limit"]
    print(f"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}")

def print_model_shardings(model, model_name="model"):
    """Iterates through the model's state and prints the sharding of each JAX array."""
    print(f"\n--- Sharding for {model_name} ---")
    state = nnx.state(model)
    flat_state = nnx.to_flat_state(state)

    if not flat_state:
        print("  Model has no state to inspect.")
        return

    for path, value in flat_state:
        if isinstance(value, nnx.VariableState) and isinstance(value.value, jax.Array):
            path_str = "/".join(map(str, path))
            print(f"  Path: {path_str}")
            print(f"    Shape: {value.value.shape}")
            print(f"    Sharding: {value.value.sharding}")
        elif isinstance(value, jax.Array): # For completeness, though state should have VariableState
            path_str = "/".join(map(str, path))
            print(f"  Path: {path_str} (direct array)")
            print(f"    Shape: {value.shape}")
            print(f"    Sharding: {value.sharding}")
    print(f"--- End of sharding for {model_name} ---\n")


"""## Load the policy model and the reference model

The policy model is the model which is actually trained and whose weights are
updated. The reference model is the model with which we compute KL divergence.
This is to ensure that the policy updates are not huge and that it does not
deviate too much from the reference model.

Typically, the reference model is the base model, and the policy model is the
same base model, but with LoRA parameters. Only the LoRA parameters are updated.

Note: We perform full precision (fp32) training. You can, however, leverage
Qwix for QAT.

To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need
to have agreed to the Gemma license
[here](https://www.kaggle.com/models/google/gemma/flax/).
"""

# # Log in
# if "KAGGLE_USERNAME" not in os.environ or "KAGGLE_KEY" not in os.environ:
#   kagglehub.login()

# kaggle_ckpt_path = kagglehub.model_download("google/gemma-2/flax/gemma2-2b-it")
kaggle_ckpt_path = kagglehub.model_download("google/gemma/flax/2b")

# # This is a workaround. The checkpoints on Kaggle don't work with NNX. So, we
# # load the model, save the checkpoint locally, and then reload the model
# # (sharded).
# params = params_lib.load_and_format_params(
#     os.path.join(kaggle_ckpt_path, "gemma2-2b-it")
# )
# gemma = gemma_lib.Transformer.from_params(params, version="2-2b-it")
# checkpointer = ocp.StandardCheckpointer()
# _, state = nnx.split(gemma)
# checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, "state"), state)

# # Wait for the ckpt to save successfully.
# time.sleep(60)

# # Delete the intermediate model to save memory.
# del params
# del gemma
# del state
# gc.collect()

# from tunix.rl import utils
# show_hbm_usage = utils.show_hbm_usage

print("HBM usage before loading model:")
show_hbm_usage()

"""### Load MaxText model"""

import sys
import os

# add the parent directory (one level up) to sys.path
sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '/home/mazumdera_google_com/maxtext')))

# ! pip install -r ../../maxtext/requirements.txt

import MaxText as mt
from MaxText import pyconfig

"""#### Convert MaxText model to nnx (use a commit from MaxText repo prior to )


"""

# from MaxText.integrations.tunix.tunix_utils import build_tunix_wrapper
from flax import linen as nn
from tunix.models.llama3 import model as llama3_lib
def get_ref_maxtext_model(config):

  #python3 -m MaxText.train MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIRECTORY} dataset_path=${DATASET_PATH} tokenizer_path=assets/tokenizer.gemma load_parameters_path=${CONVERTED_CHECKPOINT} per_device_batch_size=1 run_name=${FINETUNE_RUN_NAME} max_target_length=8192 steps=10 async_checkpointing=false model_name=gemma-2b checkpoint_period=5


  def create_model(config):
    return mt.from_pretrained(config, rngs=nnx.Rngs(params=0, dropout=1))

  abstract_model = nnx.eval_shape(create_model, config=config)
  graphdef, abstract_state = nnx.split(abstract_model)
  print('The abstract NNX state (all leaves are abstract arrays):')
  nnx.display(abstract_state)
  specs = nnx.get_partition_spec(abstract_state)
  mesh = abstract_model.mesh

  # JIT a function that creates the model state with proper sharding from the start.
  # By providing out_shardings, we instruct JAX to produce sharded output directly,
  # avoiding a large intermediate allocation on a single device.
  with nn.logical_axis_rules(config.logical_axis_rules):
    out_shardings = nn.logical_to_mesh_sharding(specs, mesh)

  @partial(jax.jit, out_shardings=out_shardings)
  def create_sharded_state():
      # This will be JIT-compiled. JAX knows the output sharding and can
      # initialize the parameters directly on the target devices in a sharded way.
      model = create_model(config)
      return nnx.state(model)

  with jax.sharding.use_mesh(mesh):
    # Create the model with sharded parameters.
    sharded_state = create_sharded_state()
    model = nnx.merge(graphdef, sharded_state)

    if config.load_parameters_path:
        target_for_restore = jax.tree.map(
            lambda v: v.value, sharded_state, is_leaf=lambda n: isinstance(n, nnx.Variable)
        )


        try:
            ckptr = ocp.Checkpointer(
                ocp.PyTreeCheckpointHandler(
                    restore_concurrent_gb=None,
                    save_concurrent_gb=None,
                    use_ocdbt=True,
                    use_zarr3=True,
                )
            )
            # This is a memory optimization. We don't want to restore the entire checkpoint - only the params.
            # Rather than pass the entire abstract state, which could unnecessarily restore opt_state and such and waste
            # memory, we instead specify here that we are just restoring the params field of the checkpoint
            # (which itself may be a dictionary containing a key named 'params').
            restore_args = ocp.checkpoint_utils.construct_restore_args(target_for_restore)
            restored = ckptr.restore(
                epath.Path(config.load_parameters_path),
                item={"params": {"params": target_for_restore}},
                transforms={},
                restore_args={"params": {"params": restore_args}},
            )
            checkpoint = restored["params"]["params"]

            # checkpoint = mt.checkpointing.load_params_from_path(
            #     load_parameters_from_path=config.load_parameters_path,
            #     abstract_unboxed_params=target_for_restore,
            #     checkpoint_storage_concurrent_gb=None,
            # )
            # checkpoint = ocp.StandardCheckpointer().restore(
            #     "gs://maxtext-gemma/2b/2025-08-05-04-37/0/items", target=target_for_restore
            # )
            if checkpoint:
                nnx.update(model, checkpoint)

        except Exception as e:
            print(f"Failed: {e}")

    tunix_model = TunixMaxTextLlama(
            base_model=model,
            use_attention_mask=False,  # trust Tunix loss masking
        )

    model_config = llama3_lib.ModelConfig.llama3_1_8b()
    tunix_model.config = model_config

  return tunix_model, mesh, model_config


from MaxText.integration.tunix.tunix_adaptor import TunixMaxTextLlama

config_ref = pyconfig.initialize(
      ["", "/home/mazumdera_google_com/maxtext/MaxText/configs/base.yml"], #TODO: @mazumdera: why decode.py?
      base_output_directory="gs://dummy_output_dir",  # This is not used in Tunix.
      run_name="test-tunix-maxtext-llama3.1-8b",
    #   run_name="test-tunix-maxtext-gemma-2b",
      # dataset_path=we use Tunix's dataset
      #TODO: @mazumdera: change this to use checkpoint
      tokenizer_type="tiktoken",
      tokenizer_path="assets/tokenizer_llama3.tiktoken",
    #   tokenizer_path="assets/tokenizer.gemma",
      load_parameters_path="gs://maxtext-model-checkpoints/llama3.1-8b/2025-01-23-19-04/scanned/0/items",
    #   load_parameters_path="gs://maxtext-gemma/2b/2025-08-05-04-37/0/items",
      per_device_batch_size=1,
      max_prefill_predict_length=4,
      max_target_length=16,
      steps=10,
      async_checkpointing="false",
      model_name="llama3.1-8b",
    #   model_name="gemma-2b",
      checkpoint_period=5,
      skip_jax_distributed_system="true",
      weight_dtype="bfloat16",
      attention="dot_product",
      remat_policy="custom",
      decoder_layer_input="offload",
      query_proj="offload",
      key_proj="offload",
      value_proj="offload",
      opt_type="sgd",
  )

llama3_1_8b, mesh, model_config = get_ref_maxtext_model(config_ref)
# gemma_maxtext_nnx = nnx.bridge.ToNNX(gemma)
# Instead of:
nnx.display(llama3_1_8b)

_maxtext_state_flatten = nnx.state(llama3_1_8b).flat_state()
maxtext_state_flatten = {
    '.'.join(str(key) for key in keys): v for keys, v in _maxtext_state_flatten
}

for key, value in maxtext_state_flatten.items():
   print(key, value.shape, value.sharding)

print(f"maxtext_state_flatten[base.token_embedder.embedding].value={maxtext_state_flatten['base.token_embedder.embedding'].value}")


# # Use:
# print("Model initialized successfully")
# print(f"Model mesh shape: {mesh.shape}")
# print(f"Model config: {model_config}")
# print("HBM usage after loading ref model:")
# show_hbm_usage()
# # # Policy model
# # lora_gemma = get_lora_model(gemma, mesh=mesh)
# # nnx.display(lora_gemma)

# import jax.numpy as jnp
# import os
# from tunix.models.gemma import gemma as gemma_lib
# from tunix.models.gemma import data as data_lib

# TEMP_BATCH_SIZE = 8

# # Assuming gemma is a pre-loaded instance of gemma_lib.Transformer
# # and data_lib is available.
# gemma_tokenizer = data_lib.GemmaTokenizer(
#     os.path.join(kaggle_ckpt_path, "tokenizer.model")
# )

# tokens = gemma_tokenizer.encode("I love to")
# # tokens.append(jnp.repeat(jnp.array([gemma_tokenizer.eos_id()]),2048-len(tokens),axis=0))  
# # tokens = [gemma_tokenizer.bos_id()]+gemma_tokenizer.encode("The color of the sky is blue but")

# repeated_tokens = jnp.repeat(jnp.array(tokens)[None, :], TEMP_BATCH_SIZE, axis=0)
# positions = jnp.repeat(jnp.arange(0, len(tokens))[None, :], TEMP_BATCH_SIZE, axis=0)

# # --- FIX STARTS HERE ---

# # The Gemma model requires an attention mask. Passing `None` causes an error.
# # We need to create a causal attention mask for the prefill step.

# # 1. Create a boolean mask for the input tokens (True for valid tokens, False for padding).
# #    Assuming `gemma_tokenizer.pad_id()` exists. If not, and there's no padding,
# #    `jnp.ones_like(repeated_tokens, dtype=jnp.bool_)` would also work.
# #    A common pad_id is 0.
# pad_id = gemma_tokenizer.pad_id()
# input_mask = (repeated_tokens != pad_id)

# # 2. Create a causal attention mask from the input mask.
# #    This prevents the model from attending to future tokens.
# attention_mask = gemma_lib.make_causal_attn_mask(input_mask)

# # 3. Call the model with the correct attention mask.
# gemma_output_logits, _ = llama3_1_8b(repeated_tokens, positions, cache=None, attention_mask=attention_mask)  # Test the model to ensure it works

# # --- FIX ENDS HERE ---

# # The commented out line below would also need a proper attention_mask.
# # For example:
# # dummy_tokens = jnp.ones((TEMP_BATCH_SIZE, 16), jnp.int32)
# # dummy_positions = jnp.repeat(jnp.arange(0,16)[None,:], TEMP_BATCH_SIZE, axis=0)
# # dummy_mask = gemma_lib.make_causal_attn_mask(jnp.ones_like(dummy_tokens, dtype=jnp.bool_))
# # gemma_output = gemma(dummy_tokens, dummy_positions, cache=None, attention_mask=dummy_mask)

# print("Successfully ran the model!")
# print("Output shape:", gemma_output_logits.shape)

# print(f"Tokens: {tokens}")
# print(f"positions: {positions}")

# last_token_logits = gemma_output_logits[:, -1, :]
# print(f"Last token logits: {last_token_logits}")
# predicted_token_id = jnp.argmax(last_token_logits, axis=-1)
# # Decode the token ID to see the predicted word.
# # Since TEMP_BATCH_SIZE is 1, we can just grab the first element.
# next_token_id = predicted_token_id[0]
# predicted_token_text = gemma_tokenizer.decode([int(next_token_id)])

# print(f"\nPredicted next token ID: {next_token_id}")
# print(f"Predicted next token: '{predicted_token_text}'")