{
  "cells": [
    {
      "metadata": {
        "id": "tO6grTnjHsWy"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/google/tunix\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "!pip uninstall -q -y flax\n",
        "!pip install --no-cache-dir git+https://github.com/google/flax.git\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q humanize"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cViWjJXNs7r-"
      },
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "kXSdCnzItA_Q"
      },
      "cell_type": "code",
      "source": [
        "import functools\n",
        "from flax import nnx\n",
        "from huggingface_hub import snapshot_download\n",
        "import humanize\n",
        "import jax\n",
        "from tunix.models.llama3 import model\n",
        "from tunix.models.llama3 import params"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "uq3z5oe4tD0X"
      },
      "cell_type": "code",
      "source": [
        "def show_hbm_usage():\n",
        "  \"\"\"Displays memory usage per device.\"\"\"\n",
        "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
        "\n",
        "  print(\"\\n--- TPU HBM Usage ---\")\n",
        "  for i, d in enumerate(jax.local_devices()):\n",
        "    stats = d.memory_stats()\n",
        "    used = stats.get(\"bytes_in_use\", 0)\n",
        "    limit = stats.get(\"bytes_limit\", 0)\n",
        "\n",
        "    hbm_used = stats.get(\"device:0:HBM0:bytes_in_use\", used)\n",
        "    hbm_limit = stats.get(\"device:0:HBM0:bytes_limit\", limit)\n",
        "\n",
        "    # Fallback if specific HBM stats not available\n",
        "    if hbm_limit == 0:\n",
        "      hbm_used = used\n",
        "      hbm_limit = limit\n",
        "\n",
        "    percentage = (hbm_used / hbm_limit * 100) if hbm_limit \u003e 0 else 0\n",
        "\n",
        "    print(\n",
        "        f\"Device {i} ({d.device_kind}): Using {fmt_size(hbm_used)} /\"\n",
        "        f\" {fmt_size(hbm_limit)} ({percentage:.2f}%)\"\n",
        "    )\n",
        "\n",
        "  print(\"--- End HBM Usage ---\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Ul-UHZhEtJs-"
      },
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-3.1-8B\"\n",
        "ignore_patterns = [\n",
        "    \"*.pth\",  # Ignore PyTorch .pth weight files\n",
        "]\n",
        "print(f\"Downloading {model_id} from Hugging Face...\")\n",
        "local_model_path = snapshot_download(\n",
        "    repo_id=model_id, ignore_patterns=ignore_patterns\n",
        ")\n",
        "print(f\"Model successfully downloaded to: {local_model_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "8OEelL5O0GJZ"
      },
      "cell_type": "code",
      "source": [
        "print(\"\\n--- HBM Usage BEFORE Model Load ---\")\n",
        "show_hbm_usage()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bWKdk2x_tLdU"
      },
      "cell_type": "code",
      "source": [
        "MODEL_CP_PATH = local_model_path\n",
        "\n",
        "config = (\n",
        "    model.ModelConfig.llama3_1_8b()\n",
        ")  # pick correponding config based on model version\n",
        "MESH = [(1, 4), (\"fsdp\", \"tp\")]\n",
        "mesh = jax.make_mesh(*MESH)\n",
        "with mesh:\n",
        "  llama = params.create_model_from_safe_tensors(MODEL_CP_PATH, config, mesh)\n",
        "  nnx.display(llama)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "4cx9mk2FtbkZ"
      },
      "cell_type": "code",
      "source": [
        "print(\"\\n--- HBM Usage AFTER Model Load ---\")\n",
        "show_hbm_usage()"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
