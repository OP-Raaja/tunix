{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3464cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8abe3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../maxtext\")))\n",
    "os.environ[\"SKIP_JAX_PRECOMPILE\"] = \"1\"\n",
    "\n",
    "import functools\n",
    "from etils import epath\n",
    "\n",
    "\n",
    "import transformers\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "from flax import nnx\n",
    "from flax import linen as nn\n",
    "\n",
    "import MaxText as mt\n",
    "from MaxText import pyconfig\n",
    "from MaxText.integration.tunix.tunix_adaptor import TunixMaxTextLlama\n",
    "\n",
    "from tunix.rl.rollout.vllm_rollout import VllmRollout\n",
    "from tunix.rl.rollout.base_rollout import RolloutConfig\n",
    "\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.models.llama3 import model as llama3_lib\n",
    "\n",
    "from vllm import LLM\n",
    "import orbax.checkpoint as ocp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a58913",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"meta-llama/Llama-3.1-8B\"\n",
    "TOTAL_TPU_TO_USE = 8\n",
    "MESH = [(1, TOTAL_TPU_TO_USE), (\"fsdp\", \"tp\")]  # YY\n",
    "\n",
    "\n",
    "model_tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL)\n",
    "mesh = model_mesh = jax.make_mesh(\n",
    "    *MESH, devices=jax.devices()[:TOTAL_TPU_TO_USE]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc99bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_maxtext_model(config):\n",
    "\n",
    "  def create_model(config):\n",
    "    return mt.from_pretrained(config, rngs=nnx.Rngs(params=0, dropout=1))\n",
    "\n",
    "  abstract_model = nnx.eval_shape(create_model, config=config)\n",
    "  graphdef, abstract_state = nnx.split(abstract_model)\n",
    "  print(\"The abstract NNX state (all leaves are abstract arrays):\")\n",
    "  nnx.display(abstract_state)\n",
    "  specs = nnx.get_partition_spec(abstract_state)\n",
    "  mesh = abstract_model.mesh\n",
    "\n",
    "  # JIT a function that creates the model state with proper sharding from the start.\n",
    "  # By providing out_shardings, we instruct JAX to produce sharded output directly,\n",
    "  # avoiding a large intermediate allocation on a single device.\n",
    "  with nn.logical_axis_rules(config.logical_axis_rules):\n",
    "    out_shardings = nn.logical_to_mesh_sharding(specs, mesh)\n",
    "\n",
    "  @functools.partial(jax.jit, out_shardings=out_shardings)\n",
    "  def create_sharded_state():\n",
    "    # This will be JIT-compiled. JAX knows the output sharding and can\n",
    "    # initialize the parameters directly on the target devices in a sharded way.\n",
    "    model = create_model(config)\n",
    "    return nnx.state(model)\n",
    "\n",
    "  with mesh:\n",
    "    # Create the model with sharded parameters.\n",
    "    sharded_state = create_sharded_state()\n",
    "    model = nnx.merge(graphdef, sharded_state)\n",
    "\n",
    "    if config.load_parameters_path:\n",
    "      target_for_restore = jax.tree.map(\n",
    "          lambda v: v.value,\n",
    "          sharded_state,\n",
    "          is_leaf=lambda n: isinstance(n, nnx.Variable),\n",
    "      )\n",
    "\n",
    "      try:\n",
    "        ckptr = ocp.Checkpointer(\n",
    "            ocp.PyTreeCheckpointHandler(\n",
    "                restore_concurrent_gb=None,\n",
    "                save_concurrent_gb=None,\n",
    "                use_ocdbt=True,\n",
    "                use_zarr3=True,\n",
    "            )\n",
    "        )\n",
    "        # This is a memory optimization. We don't want to restore the entire checkpoint - only the params.\n",
    "        # Rather than pass the entire abstract state, which could unnecessarily restore opt_state and such and waste\n",
    "        # memory, we instead specify here that we are just restoring the params field of the checkpoint\n",
    "        # (which itself may be a dictionary containing a key named 'params').\n",
    "        restore_args = ocp.checkpoint_utils.construct_restore_args(\n",
    "            target_for_restore\n",
    "        )\n",
    "        restored = ckptr.restore(\n",
    "            epath.Path(config.load_parameters_path),\n",
    "            item={\"params\": {\"params\": target_for_restore}},\n",
    "            transforms={},\n",
    "            restore_args={\"params\": {\"params\": restore_args}},\n",
    "        )\n",
    "        checkpoint = restored[\"params\"][\"params\"]\n",
    "\n",
    "        if checkpoint:\n",
    "          nnx.update(model, checkpoint)\n",
    "\n",
    "      except Exception as e:\n",
    "        raise ValueError(f\"Checkpointing failed: {e}\")\n",
    "\n",
    "    tunix_model = TunixMaxTextLlama(\n",
    "        base_model=model,\n",
    "        use_attention_mask=False,  # trust Tunix loss masking\n",
    "    )\n",
    "\n",
    "    model_config = llama3_lib.ModelConfig.llama3_1_8b()\n",
    "    tunix_model.config = model_config\n",
    "\n",
    "  return tunix_model, mesh, model_config\n",
    "\n",
    "\n",
    "from MaxText.integration.tunix.tunix_adaptor import TunixMaxTextLlama\n",
    "\n",
    "config_ref = pyconfig.initialize(\n",
    "    [\n",
    "        \"\",\n",
    "        \"../../maxtext/MaxText/configs/base.yml\",\n",
    "    ],  # TODO: @mazumdera: why decode.py?\n",
    "    base_output_directory=\"gs://dummy_output_dir\",  # This is not used in Tunix.\n",
    "    run_name=\"test-tunix-maxtext-llama3.1-8b\",\n",
    "    tokenizer_type=\"tiktoken\",\n",
    "    tokenizer_path=\"assets/tokenizer_llama3.tiktoken\",\n",
    "    load_parameters_path=\"gs://maxtext-model-checkpoints/llama3.1-8b/2025-01-23-19-04/scanned/0/items\",\n",
    "    per_device_batch_size=1,\n",
    "    max_prefill_predict_length=4,\n",
    "    max_target_length=16,\n",
    "    steps=10,\n",
    "    async_checkpointing=\"false\",\n",
    "    model_name=\"llama3.1-8b\",\n",
    "    checkpoint_period=5,\n",
    "    skip_jax_distributed_system=\"true\",\n",
    "    weight_dtype=\"bfloat16\",\n",
    "    attention=\"dot_product\",\n",
    "    remat_policy=\"custom\",\n",
    "    decoder_layer_input=\"offload\",\n",
    "    query_proj=\"offload\",\n",
    "    key_proj=\"offload\",\n",
    "    value_proj=\"offload\",\n",
    "    opt_type=\"sgd\",\n",
    ")\n",
    "\n",
    "maxtext_model, _, model_config = get_ref_maxtext_model(config_ref)\n",
    "nnx.display(maxtext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3521804",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_GENERATION_STEPS = 64\n",
    "MAX_PROMPT_LENGTH = 64\n",
    "TEMPERATURE = 0.9\n",
    "TOP_P = 1.0\n",
    "TOP_K = None\n",
    "cache_config = base_rollout.RolloutConfig(\n",
    "    max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "    kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=TOP_P,\n",
    "    top_k=TOP_K,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53216a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_rollout = VllmRollout(\n",
    "    init_with_random_weights=False,\n",
    "    hbm_utilization=0.3,\n",
    "    tpu_backend_type=\"tpu\",\n",
    "    model=maxtext_model,\n",
    "    tokenizer=model_tokenizer,\n",
    "    cache_config_or_size=1024,\n",
    "    mesh=mesh,\n",
    "    lora_config=None,\n",
    "    model_version=MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d62d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = vllm_rollout.generate(\n",
    "    [\"The capital of France is\"],\n",
    "    rollout_config=RolloutConfig(\n",
    "        n=1, max_tokens_to_generate=64, temperature=0.1\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf7b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
